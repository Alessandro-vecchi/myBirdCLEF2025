{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"},{"sourceId":11875127,"sourceType":"datasetVersion","datasetId":7463065},{"sourceId":12062978,"sourceType":"datasetVersion","datasetId":7592759},{"sourceId":12063284,"sourceType":"datasetVersion","datasetId":7592967},{"sourceId":12065091,"sourceType":"datasetVersion","datasetId":7594128},{"sourceId":239793644,"sourceType":"kernelVersion"},{"sourceId":240246005,"sourceType":"kernelVersion"},{"sourceId":242934198,"sourceType":"kernelVersion"},{"sourceId":242936235,"sourceType":"kernelVersion"},{"sourceId":243964920,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n\n# --- Install offline packages ---\ntry:\n    import ace_tools_open\nexcept ModuleNotFoundError:\n    print('Installing ace tools...')\n    !pip install -q /kaggle/input/offline-packages/itables-2.3.0-py3-none-any.whl\n    !pip install -q /kaggle/input/offline-packages/ace_tools_open-0.1.0-py3-none-any.whl\n    \ntry:\n    import timm\nexcept ModuleNotFoundError:\n    print('Installing timm...')\n    !pip install -q /kaggle/input/offline-packages/timm-1.0.15-py3-none-any.whl\n    \n# --- Core libraries ---\nimport os\nimport math\nimport random\nimport time\nimport numpy as np\nimport seaborn as sns\nfrom collections import defaultdict\nimport plotly.express as px\n\n# --- Data handling ---\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, average_precision_score, precision_recall_curve\n\nfrom dataclasses import dataclass, field\nfrom sklearn.preprocessing import label_binarize\n\n# --- PyTorch ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchaudio.functional import bandpass_biquad\n\n# --- Audio processing ---\nimport torchaudio\nimport torchaudio.transforms as T\n\n# --- Visualization ---\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Audio, display\nfrom tqdm.notebook import tqdm\nimport ace_tools_open as tools\nimport torchvision\nfrom torchvision.ops.focal_loss import sigmoid_focal_loss\nimport cv2\n\n# --- Parallel and Custom Tools ---\nfrom joblib import Parallel, delayed\nfrom torch.amp import GradScaler, autocast\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom typing import Optional, List, Tuple\nimport timm\nimport tempfile\nimport gc\nimport itertools\nfrom glob import glob\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:14:57.547807Z","iopub.execute_input":"2025-06-10T18:14:57.547967Z","iopub.status.idle":"2025-06-10T18:14:57.597442Z","shell.execute_reply.started":"2025-06-10T18:14:57.547955Z","shell.execute_reply":"2025-06-10T18:14:57.596733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = os.listdir(\"/kaggle/input/eda-birdclef2025\") + os.listdir(\"/kaggle/input/precomputing-spectrograms\") + os.listdir(\"/kaggle/input/precomputing-spectrograms2\") + os.listdir(\"/kaggle/input/precomputing-spectrograms3\")\nf = [g for g in n if g.endswith(\".csv\")]\nf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.501911Z","iopub.execute_input":"2025-06-10T14:59:09.502361Z","iopub.status.idle":"2025-06-10T14:59:09.524940Z","shell.execute_reply.started":"2025-06-10T14:59:09.502338Z","shell.execute_reply":"2025-06-10T14:59:09.524213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass CFG:\n    # General\n    LOAD_DATA: bool = True\n    seed: int = 69\n    debug: bool = False\n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n    BATCH_SIZE: int = 32\n    num_workers: int = 4\n    \n    ## Data paths ##\n    OUTPUT_DIR: str = '/kaggle/working/'\n    temporary_dir: str = field(init=False)\n    spectrogram_dir: str = \"/kaggle/input/precomputing-spectrograms3\" # \"/kaggle/input/eda-birdclef2025\" # \"/kaggle/input/precomputing-spectrograms\" #\"/kaggle/working/precomputed_spectograms\"\n    spectrogram_csv_filename: str = \"spec_metadata.csv\"\n    spectrograms_metadata_path: str = field(init=False) # \"filename\", \"num_frames\", \"primary_label\"\n    \n    # Base path to dataset\n    data_path: str = '/kaggle/input/birdclef-2025/'\n    # Key file paths\n    metadata_path: str = field(init=False)\n    taxonomy_path: str = field(init=False)\n    sample_submission_path: str = field(init=False)\n    location_path: str = field(init=False)\n    # Audio data directories\n    train_data_path: str = field(init=False)\n    test_soundscapes_path: str = field(init=False)\n    unlabeled_soundscapes_path: str = field(init=False)\n\n    # Augmentation\n    augment = False\n    mixup = False\n    aug_prob: float = 0.5\n    mixup_alpha: float = 0.4\n\n    # Model\n    model_name: str = \"efficientnet_b0\" # 'efficientnet_b3_pruned', 'efficientnetv2_rw_m', 'efficientvit_l1', 'efficientvit_l2', 'efficientvit_m0'\n    pretrained: bool = True\n    input_directory: str = '/kaggle/input/offline-packages'\n    input_model_filename: str = field(init=False)\n    \n    timewise_weights_path: str = '/kaggle/input/effnet28/efficientnet_b0_sed.pth'\n    freqwise_weights_path: str = '/kaggle/input/effnet14/efficientnet_b0_sed.pth'\n    \n    output_model_filename: str = field(init=False)\n    custom_weights_path: str = field(init=False)\n    model_weights: str = field(init=False)\n    num_classes: int = field(init=False)\n\n    def __post_init__(self):\n        self.metadata_path = os.path.join(self.data_path, 'train.csv')\n        self.taxonomy_path = os.path.join(self.data_path, 'taxonomy.csv')\n        self.sample_submission_path = os.path.join(self.data_path, 'sample_submission.csv')\n        self.location_path = os.path.join(self.data_path, 'recording_location.txt')\n        self.train_data_path = os.path.join(self.data_path, 'train_audio')\n        self.test_soundscapes_path = os.path.join(self.data_path, 'test_soundscapes')\n        self.unlabeled_soundscapes_path = os.path.join(self.data_path, 'train_soundscapes')\n        self.spectrograms_metadata_path = os.path.join(self.spectrogram_dir, self.spectrogram_csv_filename)\n        \n        self.input_model_filename = f'{self.model_name}_pretrained.pth'\n        self.output_model_filename = f'{self.model_name}_sed.pth'\n        self.model_weights = os.path.join(self.input_directory, self.input_model_filename)\n        self.custom_weights_path = f'/kaggle/input/effnet31/{self.output_model_filename}'\n        self.num_classes = len(pd.read_csv(self.taxonomy_path))\n\n        self.temporary_dir = tempfile.TemporaryDirectory().name\n        if self.debug:\n            self.EPOCHS = 2\n            print(\"âš ï¸ Debug mode is ON. Training only for 2 epochs.\")\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.525820Z","iopub.execute_input":"2025-06-10T14:59:09.526040Z","iopub.status.idle":"2025-06-10T14:59:09.624540Z","shell.execute_reply.started":"2025-06-10T14:59:09.526024Z","shell.execute_reply":"2025-06-10T14:59:09.623925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"class PrecomputedSpectrogramDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading precomputed log-mel spectrograms.\n\n    Arguments:\n    ----------\n    metadata : pd.DataFrame\n        DataFrame containing filenames, primary labels, and number of frames.\n    spec_dir : str\n        Directory where the spectrogram .npy files are saved.\n    label_to_index : dict\n        Mapping from class name to label index.\n    augment : bool\n        Whether to apply augmentation (placeholder for now).\n    \"\"\"\n    def __init__(self, metadata, spec_dir, augment=False):\n        self.metadata = metadata\n        self.spectrogram_dir = spec_dir\n        self.label_to_class, self.label_to_index, _, self.filename_to_secondary_label = get_mappings()\n        self.augment = augment\n        if self.augment:\n            self.augmentor = SpectrogramAugmentor()\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        path = os.path.join(self.spectrogram_dir, \"precomputed_spectrograms\", row[\"filename\"])\n        \n        # Load spectrogram\n        spec = np.load(path, mmap_mode=\"r\")\n        spec = torch.tensor(spec, dtype=torch.float16).unsqueeze(0)  # [1, M, T]\n        \n        # Get the class name\n        primary_label = row[\"primary_label\"]\n        class_name = self.label_to_class.get(primary_label, \"Unknown\")\n        \n        # Apply augmentations if enabled\n        if self.augment and class_name in {\"Aves\", \"Insecta\", \"Amphibia\", \"Mammalia\"}:\n            spec = self.augmentor.apply_augmentations(spec, class_name)\n        \n        # Convert to 3-channel image-like format\n        spec = spec.repeat(3, 1, 1)\n        \n        # Multi-hot vector for primary labels\n        primary_label_tensor = torch.zeros(len(self.label_to_index), dtype=torch.float16)\n        if primary_label in self.label_to_index:\n            primary_label_tensor[self.label_to_index[primary_label]] = 1.0\n        \n        # Multi-hot vector for secondary labels\n        secondary_labels = self.filename_to_secondary_label.get(f\"{row['filename'].split('_')[0]}.ogg\", [])\n        secondary_label_tensor = torch.zeros(len(self.label_to_index), dtype=torch.float16)\n        for sec_label in secondary_labels:\n            if sec_label in self.label_to_index:\n                secondary_label_tensor[self.label_to_index[sec_label]] = 0.5\n    \n        # Combine them (primary gets 1.0 weight, secondary gets 0.5 weight)\n        combined_labels = primary_label_tensor + secondary_label_tensor\n        combined_labels = torch.clamp(combined_labels, 0, 1)  # Ensure it's only 0 or 1\n        \n        return {\n            \"spectrogram\": spec, \n            \"labels\": combined_labels, \n            \"filename\": row[\"filename\"], \n            \"class_name\": class_name\n        }\ndef collate_fn(batch, mixup=False, alpha=0.4):\n    \"\"\"\n    Custom collate function to handle varying-size spectrograms and apply Mixup if specified.\n\n    Parameters:\n    -----------\n    batch : list\n        List of samples (dict) with spectrogram, label, and filename.\n    mixup : bool\n        Whether to apply Mixup augmentation to the batch.\n    alpha : float\n        Alpha parameter for the Beta distribution in Mixup.\n\n    Returns:\n    --------\n    dict : \n        Dictionary with stacked tensors and filenames.\n    \"\"\"\n    if len(batch) == 0:\n        return {\"spectrograms\": None, \"labels\": None, \"filenames\": None}\n    \n    # Extract elements\n    specs = [item[\"spectrogram\"] for item in batch]\n    labels = [item[\"labels\"] for item in batch]\n    filenames = [item[\"filename\"] for item in batch]\n\n    # Stack along the batch dimension\n    specs = torch.stack(specs)\n    labels = torch.stack(labels)\n\n    # ðŸš€ Apply Mixup if specified and more than one element exists in the batch\n    if mixup and len(batch) > 1:\n        indices = torch.randperm(len(batch))\n        mixed_specs = []\n        mixed_labels = []\n\n        for i in range(0, len(batch) - 1, 2):\n            lam = np.random.beta(alpha, alpha)\n            spec1, spec2 = specs[i], specs[indices[i]]\n            label1, label2 = labels[i], labels[indices[i]]\n\n            mixed_spec = lam * spec1 + (1 - lam) * spec2\n            mixed_label = lam * label1 + (1 - lam) * label2\n\n            mixed_specs.append(mixed_spec)\n            mixed_labels.append(mixed_label)\n\n        # If the batch size is odd, we append the last sample as it is\n        if len(batch) % 2 != 0:\n            mixed_specs.append(specs[-1])\n            mixed_labels.append(labels[-1])\n\n        specs = torch.stack(mixed_specs)\n        labels = torch.stack(mixed_labels)\n\n    return {\"spectrograms\": specs, \"labels\": labels, \"filenames\": filenames}\n\ndef get_mappings():\n    \"\"\"\n    Creates label-to-class and label-to-index mappings.\n    Also maps filenames to their associated secondary labels.\n    \n    Returns:\n    --------\n    - label_to_class : dict\n        Maps primary labels to their respective class names.\n    - label_to_index : dict\n        Maps primary labels to unique index values.\n    - filename_to_secondary_label : dict\n        Maps filenames to lists of secondary labels.\n    \"\"\"\n    # Load the datasets\n    taxonomy_df = pd.read_csv(cfg.taxonomy_path)\n    metadata_df = pd.read_csv(cfg.spectrograms_metadata_path)\n    train_df = pd.read_csv(cfg.metadata_path)\n    \n    # Filter out unused labels\n    used_labels = set(metadata_df[\"primary_label\"].unique())\n    taxonomy_df = taxonomy_df[taxonomy_df['primary_label'].isin(used_labels)]\n\n    # Label to Class Mapping\n    label_to_class = taxonomy_df.set_index('primary_label')['class_name'].to_dict()\n\n    # Label to Index Mapping â†’ Now guaranteed to match!\n    label_to_index = {label: idx for idx, label in enumerate(sorted(label_to_class.keys()))}\n    index_to_label = {idx: label for idx, label in enumerate(sorted(label_to_class.keys()))}\n\n    # ðŸŒŸ New Logic to retrieve filename â†’ secondary_labels\n    filename_to_secondary_label = {}\n\n    # Iterate over the DataFrame\n    for _, row in train_df.iterrows():\n        filename = row[\"filename\"]\n        secondary_labels = eval(row[\"secondary_labels\"]) if isinstance(row[\"secondary_labels\"], str) and row[\"secondary_labels\"] != \"['']\" else []\n\n        # Only add if there are secondary labels\n        if len(secondary_labels) > 0:\n            filename_to_secondary_label[filename] = secondary_labels\n\n    # Display the label maps only once (no need to repeat every call)\n    if not hasattr(get_mappings, \"_displayed\"):\n        try:\n            tools.display_dataframe_to_user(name=\"Label to Class\", dataframe=pd.DataFrame(label_to_class.items(), columns=[\"Animal Label\", \"Class Name\"]))\n            tools.display_dataframe_to_user(name=\"Label Map\", dataframe=pd.DataFrame(label_to_index.items(), columns=[\"Animal Label\", \"Index\"]))\n            tools.display_dataframe_to_user(name=\"Filename to Secondary Labels\", dataframe=pd.DataFrame(list(filename_to_secondary_label.items()), columns=[\"Filename\", \"Secondary Labels\"]))\n            get_mappings._displayed = True\n        except NameError as e:\n            print(e)\n    \n    return label_to_class, label_to_index, index_to_label, filename_to_secondary_label\n\ndef create_dataloader(dataset, cfg, shuffle, collate_fn):\n    \"\"\"\n    Creates a DataLoader for a given dataset.\n\n    Parameters:\n    -----------\n    dataset : Dataset\n        PyTorch dataset object.\n    cfg : object\n        Configuration object containing batch size and num_workers.\n    shuffle : bool\n        Whether to shuffle the dataset.\n    collate_fn : callable\n        Collate function for batching.\n\n    Returns:\n    --------\n    DataLoader:\n        A PyTorch DataLoader instance.\n    \"\"\"\n    loader = DataLoader(\n        dataset,\n        batch_size=cfg.BATCH_SIZE,\n        shuffle=shuffle,\n        num_workers=cfg.num_workers,\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n    return loader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.625355Z","iopub.execute_input":"2025-06-10T14:59:09.625571Z","iopub.status.idle":"2025-06-10T14:59:09.641921Z","shell.execute_reply.started":"2025-06-10T14:59:09.625547Z","shell.execute_reply":"2025-06-10T14:59:09.641254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SED models","metadata":{}},{"cell_type":"code","source":"class EfficientNetTimeSED(nn.Module):\n    \"\"\"\n    EfficientNet with an SED Head for BirdCLEF inference.\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        \n        # Backbone with no classifier head\n        self.backbone = timm.create_model(\n            cfg.model_name, \n            pretrained=False, \n            features_only=False\n        )\n        # Load pretrained weights\n        print(f\"[INFO] Loading custom trained weights from {cfg.model_weights}...\")\n        checkpoint = torch.load(cfg.model_weights, map_location=cfg.device, weights_only=True)\n        self.backbone.load_state_dict(checkpoint)\n\n        # Remove classifier and add custom head\n        self.feature_dim = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        \n        # Attention Block\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, None))\n        self.conv_att = nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n        # Final Classification Layer\n        self.classifier = nn.Sequential(\n            nn.Conv2d(self.feature_dim, cfg.num_classes, kernel_size=1),\n            nn.AdaptiveMaxPool2d((1, 1)),\n            nn.Flatten()\n        )\n\n        self.attention_weights = None \n        \n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the model.\n        Parameters:\n        -----------\n        x : Tensor of shape [B, 3, M, T] where M = Mel bands, T = Time.\n        \n        Returns:\n        --------\n        Tensor of shape [B, num_classes]\n        \"\"\"\n        features = self.backbone.forward_features(x)  # shape: [B, C, M', T']\n        \n        attn = self.avg_pool(features)                # â†’ [B, C, 1, T]\n        attn = self.conv_att(attn)                    # pointwise conv\n        attn = self.sigmoid(attn)                     # sigmoid scaling [B, C, 1, T]\n\n        self.attention_weights = attn.detach()        # Store for visualization\n        features = features * attn                    # Apply attention\n\n        out = self.classifier(features)               # Final output\n        return out\n\n    def get_framewise_output(self, x):\n        features = self.backbone.forward_features(x)  # [B, C, F, T]\n        attn = self.sigmoid(self.conv_att(self.avg_pool(features)))  # [B, C, 1, T]\n        weighted = features * attn  # [B, C, F, T]\n        raw_output = self.classifier[0](weighted)  # Conv2d only: [B, num_classes, F, T]\n        framewise_output = raw_output.mean(dim=2)  # Mean over F â†’ [B, num_classes, T]\n        return self.sigmoid(framewise_output)\n\n\ndef remap_attention_keys(state_dict):\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith(\"att_block.1.\"):\n            new_k = k.replace(\"att_block.1\", \"conv_att\")\n        elif k.startswith(\"att_block.2.\"):\n            new_k = k.replace(\"att_block.2\", \"sigmoid\")\n        else:\n            new_k = k\n        new_state_dict[new_k] = v\n    return new_state_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.643738Z","iopub.execute_input":"2025-06-10T14:59:09.643956Z","iopub.status.idle":"2025-06-10T14:59:09.660109Z","shell.execute_reply.started":"2025-06-10T14:59:09.643940Z","shell.execute_reply":"2025-06-10T14:59:09.659407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EfficientNetFrequencySED(nn.Module):\n    \"\"\"\n    EfficientNet with a custom SED head for frequency-wise attention.\n    \n    This model:\n    - Uses a pretrained EfficientNet backbone\n    - Applies a frequency-wise attention mechanism\n    - Outputs class probabilities for multi-class classification\n    \n    Arguments:\n    ----------\n    cfg : object\n        Configuration object (assumes it's an instance of CFG)\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        \n        # Store config and device\n        self.cfg = cfg\n        self.device = torch.device(cfg.device)\n\n        # Create model with the correct architecture\n        self.backbone = timm.create_model(cfg.model_name, pretrained=cfg.pretrained)\n\n        # Remove classifier head, we will add our own\n        self.feature_dim = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()  # Remove classifier\n\n        # Frequency-wise attention block -> attention mechanism to emphasize important frequency regions.\n        self.att_block = nn.Sequential(\n            nn.AdaptiveAvgPool2d((None, 1)),          # Mean over frequency bands\n            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n        # Custom classifier head\n        self.classifier = nn.Sequential(\n            nn.Conv2d(self.feature_dim, cfg.num_classes, kernel_size=1),\n            nn.AdaptiveMaxPool2d((1, 1)),\n            nn.Flatten()\n        )\n\n        self.attention_weights = None \n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor of shape [B, 3, M, T], where:\n            - B = Batch size\n            - M = Mel bands (frequency bins)\n            - T = Time frames\n\n        Returns:\n        --------\n        torch.Tensor:\n            Output tensor of shape [B, num_classes]\n        \"\"\"\n        x = x.to(self.device)\n        features = self.backbone.forward_features(x)  # EfficientNet backbone [B, C, M', T']\n        attn = self.att_block(features)  # Attention on frequency bands [B, C, T', 1]\n\n        self.attention_weights = attn.detach()        # Store for visualization\n        features = features * attn       # Apply attention\n        \n        out = self.classifier(features)  # Classify [B, num_classes]\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.660834Z","iopub.execute_input":"2025-06-10T14:59:09.661005Z","iopub.status.idle":"2025-06-10T14:59:09.679309Z","shell.execute_reply.started":"2025-06-10T14:59:09.660991Z","shell.execute_reply":"2025-06-10T14:59:09.678625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model evaluation and interpretation","metadata":{}},{"cell_type":"code","source":"class ModelEvaluator:\n    def __init__(self, model, dataloader, cfg, label_to_class, index_to_label):\n        self.model = model\n        self.dataloader = dataloader\n        self.cfg = cfg\n        self.label_to_class = label_to_class\n        self.index_to_label = index_to_label\n        self.num_classes = len(index_to_label)\n        \n    def run_inference(self):\n        all_targets, all_preds = [], []\n        self.model.eval()\n        \n        with torch.no_grad():\n            for batch in tqdm(self.dataloader, desc=\"Evaluating\"):\n                inputs = batch[\"spectrograms\"].to(self.cfg.device)\n                targets = batch[\"labels\"].to(self.cfg.device)\n                \n                # Only keep primary labels for AUC calculation\n                primary_only_targets = (targets == 1).int()\n                assert primary_only_targets.sum() == primary_only_targets.shape[0]\n                \n                try:\n                    with autocast(device_type=self.cfg.device):\n                        outputs = self.model(inputs)\n                        outputs = torch.sigmoid(outputs).detach().cpu().numpy()\n                except Exception as e:\n                    print(f\"[WARNING] Exception during inference: {e}\")\n                    outputs = self.model(inputs.float())\n                    outputs = torch.sigmoid(outputs).cpu().numpy()\n                \n                all_targets.append(primary_only_targets.cpu().numpy())\n                #all_targets.append(targets.cpu().numpy())\n                all_preds.append(outputs)\n        \n        y_true = np.concatenate(all_targets, axis=0)\n        y_pred = np.concatenate(all_preds, axis=0)\n        return y_true, y_pred\n\n    def calculate_per_class_auc(self, y_true, y_pred):\n        results = []\n        for idx in range(self.num_classes):\n            true_class = y_true[:, idx]\n            pred_class = y_pred[:, idx]\n\n            if np.sum(true_class) > 0 and np.sum(true_class) < len(true_class):\n                try:\n                    auc = roc_auc_score(true_class, pred_class)\n                except ValueError:\n                    auc = np.nan\n            else:\n                auc = np.nan\n\n            species = self.index_to_label[idx]\n            class_name = self.label_to_class[species]\n\n            results.append({\n                \"species\": species,\n                \"class_name\": class_name,\n                \"auc\": auc,\n                \"positive_samples\": int(np.sum(true_class)),\n                \"total_samples\": len(true_class)\n            })\n        \n        df_results = pd.DataFrame(results)\n        return df_results\n\n    def plot_results(self, df_results):\n        plt.figure(figsize=(15, 6))\n        sns.barplot(\n            x=\"species\", y=\"auc\", hue=\"class_name\", \n            data=df_results.sort_values(by=\"auc\"),\n            dodge=False\n        )\n        plt.xticks(rotation=90, fontsize=6)\n        plt.title(\"Per-Class ROC AUC Scores\")\n        plt.xlabel(\"Species\")\n        plt.ylabel(\"AUC\")\n        plt.legend(title=\"Class\", loc=\"lower right\")\n        plt.tight_layout()\n        plt.show()\n\n        plt.figure(figsize=(8, 5))\n        sns.histplot(df_results[\"auc\"].dropna(), bins=20, kde=True)\n        plt.title(\"Distribution of Per-Class AUC Scores\")\n        plt.xlabel(\"AUC Score\")\n        plt.ylabel(\"Frequency\")\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:41:38.173417Z","iopub.execute_input":"2025-06-10T15:41:38.173685Z","iopub.status.idle":"2025-06-10T15:41:38.187223Z","shell.execute_reply.started":"2025-06-10T15:41:38.173664Z","shell.execute_reply":"2025-06-10T15:41:38.186626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ModelAnalyzer:\n    def __init__(self, df_results, y_true, y_pred, do_print=False):\n        self.df_results = df_results.copy()\n        self.y_true = y_true  # shape (n_samples, n_classes), one-hot encoded\n        self.y_pred = y_pred  # shape (n_samples, n_classes), probabilities\n        self.print = do_print\n        \n    def summarize_best_worst(self, top_n=10):\n        df_valid = self.df_results.dropna(subset=[\"auc\"])\n        best = df_valid.sort_values(by=\"auc\", ascending=False).head(top_n)\n        worst = df_valid.sort_values(by=\"auc\", ascending=True).head(top_n)\n        \n        if self.print:\n            print(\"\\n=== BEST CLASSES ===\")\n            print(best[[\"species\", \"class_name\", \"auc\", \"positive_samples\"]])\n            print(\"\\n=== WORST CLASSES ===\")\n            print(worst[[\"species\", \"class_name\", \"auc\", \"positive_samples\"]])\n\n    def compute_accuracy(self):\n        y_true_labels = np.argmax(self.y_true, axis=1)\n        y_pred_labels = np.argmax(self.y_pred, axis=1)\n        overall_acc = accuracy_score(y_true_labels, y_pred_labels)\n        if self.print:\n            print(f\"\\n=== Overall Accuracy: {overall_acc:.4f} ===\")\n\n        # Per-class accuracy\n        correct_per_class = []\n        total_per_class = []\n\n        for idx in range(self.y_true.shape[1]):\n            true_class_idx = np.where(y_true_labels == idx)[0]\n            if len(true_class_idx) == 0:\n                acc = np.nan\n            else:\n                acc = np.mean(y_pred_labels[true_class_idx] == idx)\n            correct_per_class.append(acc)\n            total_per_class.append(len(true_class_idx))\n\n        self.df_results[\"accuracy\"] = correct_per_class\n        \n        if self.print:\n            print(\"\\n=== Per-Class Accuracy Added ===\")\n            print(self.df_results[[\"species\", \"class_name\", \"accuracy\"]].sort_values(by=\"accuracy\", ascending=True))\n        return self.df_results\n\n    def compute_auc(self):\n        aucs = []\n        for idx in range(self.y_true.shape[1]):\n            true_class = self.y_true[:, idx]\n            pred_class = self.y_pred[:, idx]\n            try:\n                auc = roc_auc_score(true_class, pred_class)\n            except ValueError:\n                auc = np.nan\n            aucs.append(auc)\n        self.df_results[\"auc\"] = aucs\n         \n        if self.print:\n            print(\"\\n=== AUC Recomputed Without Positive/Negative Checks ===\")\n            print(self.df_results[[\"species\", \"class_name\", \"auc\"]].sort_values(by=\"auc\", ascending=True))\n        return self.df_results\n\n    def compute_precision_recall(self):\n        precision_results = []\n        for idx in range(self.y_true.shape[1]):\n            true_class = self.y_true[:, idx]\n            pred_class = self.y_pred[:, idx]\n            try:\n                ap = average_precision_score(true_class, pred_class)\n            except ValueError:\n                ap = np.nan\n            precision_results.append(ap)\n        self.df_results[\"average_precision\"] = precision_results\n        \n        if self.print:\n            print(\"\\n=== Precision-Recall Added ===\")\n            print(self.df_results[[\"species\", \"class_name\", \"average_precision\"]].sort_values(by=\"average_precision\", ascending=True))\n        return self.df_results\n\n    def compute_confusion_stats(self, threshold=0.5):\n        false_pos = []\n        false_neg = []\n        for idx in range(self.y_true.shape[1]):\n            true_class = self.y_true[:, idx]\n            pred_class = (self.y_pred[:, idx] >= self.df_results[\"best_threshold\"][idx]).astype(int)\n            fp = np.sum((pred_class == 1) & (true_class == 0))\n            fn = np.sum((pred_class == 0) & (true_class == 1))\n            false_pos.append(fp)\n            false_neg.append(fn)\n        self.df_results[\"false_positives\"] = false_pos\n        self.df_results[\"false_negatives\"] = false_neg\n        \n        if self.print:\n            print(\"\\n=== Confusion Stats Added ===\")\n            print(self.df_results[[\"species\", \"class_name\", \"false_positives\", \"false_negatives\"]].sort_values(by=\"false_negatives\", ascending=False))\n        return self.df_results\n\n    def compute_best_thresholds(self):\n        best_thresholds = []\n        for idx in range(self.y_true.shape[1]):\n            y_true_class = self.y_true[:, idx]\n            y_pred_class = self.y_pred[:, idx]\n            precision, recall, thresholds = precision_recall_curve(y_true_class, y_pred_class)\n            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n            if len(thresholds) > 0:\n                best_thresh = thresholds[np.argmax(f1_scores)]\n            else:\n                best_thresh = 0.5  # fallback default\n            best_thresholds.append(best_thresh)\n        self.df_results[\"best_threshold\"] = best_thresholds\n        \n        if self.print:\n            print(\"\\n=== Best Thresholds Computed per Class ===\")\n            print(self.df_results[[\"species\", \"class_name\", \"best_threshold\"]].sort_values(by=\"best_threshold\", ascending=True))\n        return self.df_results\n\n    def get_species_info(self, primary_label):\n        row = self.df_results[self.df_results[\"species\"] == primary_label]\n        if row.empty:\n            print(f\"[WARNING] Species {primary_label} not found in results.\")\n            return None\n        info = row[[\n            \"species\", \"class_name\", \"auc\", \"positive_samples\",\n            \"average_precision\", \"false_positives\", \"false_negatives\", \"accuracy\", \"total_samples\"\n        ]]\n        tools.display_dataframe_to_user(name=f\"\\n=== Info for Species '{primary_label}' ===\", dataframe=pd.DataFrame(info))\n        return info\n\n    def display_full_dataframe(self):\n        tools.display_dataframe_to_user(name=f\"\\n=== Full Results DataFrame ===\", dataframe=self.df_results)\n\n    def plot_stats(self, df_with_confusion, n=10):\n        plt.figure(figsize=(8, 6))\n        sns.scatterplot(\n            x=\"false_positives\", y=\"false_negatives\",\n            hue=\"class_name\", size=\"positive_samples\",\n            data=df_with_confusion, alpha=0.7, edgecolor='k'\n        )\n        plt.title(\"False Positives vs False Negatives per Class\")\n        plt.xlabel(\"False Positives\")\n        plt.ylabel(\"False Negatives\")\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.tight_layout()\n        plt.show()\n\n        plt.figure(figsize=(8, 6))\n        sns.scatterplot(\n            x=\"positive_samples\", y=\"auc\",\n            hue=\"class_name\",\n            data=df_with_confusion, alpha=0.7, edgecolor='k'\n        )\n        plt.title(\"AUC vs Positive Sample Count\")\n        plt.xlabel(\"Positive Samples\")\n        plt.ylabel(\"AUC\")\n        plt.xscale('log')\n        plt.tight_layout()\n        plt.show()\n\n        top_fn = df_with_confusion.sort_values(by=\"false_negatives\", ascending=False).head(n)\n        if self.print:\n            print(f\"\\n=== Top {n} Classes with Most False Negatives ===\")\n            print(top_fn[[\"species\", \"class_name\", \"false_negatives\", \"false_positives\"]])\n\n        for idx, row in top_fn.iterrows():\n            y_true_class = self.y_true[:, idx]\n            y_pred_class = self.y_pred[:, idx]\n            precision, recall, thresholds = precision_recall_curve(y_true_class, y_pred_class)\n            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n            best_thresh = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else 0.5\n            if self.print:\n                print(f\"Species: {row['species']} | Best Threshold: {best_thresh:.3f}\")\n                \n    def plot_performance_metrics(self):\n        df_acc = self.df_results.sort_values(by=\"accuracy\", ascending=True)\n        df_ap = self.df_results.sort_values(by=\"average_precision\", ascending=True)\n    \n        # Use integer indices for x-axis to prevent label clutter\n        df_acc[\"index\"] = range(len(df_acc))\n        df_ap[\"index\"] = range(len(df_ap))\n\n        # Generate a fixed color mapping\n        unique_classes = self.df_results[\"class_name\"].unique()\n        palette = px.colors.qualitative.Safe  # or use D3, Set1, etc.\n        color_map = {cls: palette[i % len(palette)] for i, cls in enumerate(sorted(unique_classes))}\n\n    \n        # Accuracy Plot\n        fig1 = px.bar(\n            df_acc,\n            x=\"index\",\n            y=\"accuracy\",\n            color=\"class_name\",\n            color_discrete_map=color_map,\n            hover_data=[\"species\", \"class_name\", \"accuracy\", \"positive_samples\"],\n            title=\"Accuracy by Species (Hover to See Species)\",\n            labels={\"index\": \"Class Index\", \"accuracy\": \"Accuracy\", \"positive_samples\": \"# samples\", \"class_name\": \"Class name\"},\n            height=500\n        )\n        fig1.update_layout(\n            xaxis_tickvals=[],  # Remove tick labels\n            showlegend=True,\n            legend_title=\"Class Name\"\n        )\n        fig1.show()\n    \n        # Average Precision Plot\n        fig2 = px.bar(\n            df_ap,\n            x=\"index\",\n            y=\"average_precision\",\n            color=\"class_name\",\n            color_discrete_map=color_map,\n            hover_data=[\"species\", \"class_name\", \"average_precision\", \"positive_samples\"],\n            title=\"Average Precision by Species (Hover to See Species)\",\n            labels={\"index\": \"Class Index\", \"average_precision\": \"Average Precision\", \"positive_samples\": \"# samples\", \"class_name\": \"Class name\"},\n            height=500\n        )\n        fig2.update_layout(\n            xaxis_tickvals=[],\n            showlegend=True,\n            legend_title=\"Class Name\"\n        )\n        fig2.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:29.475661Z","iopub.execute_input":"2025-06-10T15:53:29.475934Z","iopub.status.idle":"2025-06-10T15:53:29.500504Z","shell.execute_reply.started":"2025-06-10T15:53:29.475916Z","shell.execute_reply":"2025-06-10T15:53:29.499906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def select_species_samples(\n    loader,\n    species: str,\n    mode: str = \"random\",\n    num_samples: int = 5,\n    target_filename: Optional[str] = None\n) -> List[Tuple[torch.Tensor, str]]:\n    \"\"\"\n    Selects samples from the loader for a given species.\n\n    Parameters:\n    -----------\n    loader : DataLoader\n        The data loader to pull samples from.\n    species : str\n        The species ID to match in filenames.\n    mode : str\n        Selection mode: \"random\", \"consecutive\", or \"specific\".\n    num_samples : int\n        Number of samples to select.\n    target_filename : str, optional\n        Used only when mode == \"specific\".\n\n    Returns:\n    --------\n    List of tuples: (spectrogram tensor, filename)\n    \"\"\"\n    # Get species info from analyzer\n    row = analyzer.get_species_info(species)\n    max_n = row[\"positive_samples\"].item()\n    num_samples = min(max_n, num_samples)\n    \n    all_matches = []\n\n    for batch in loader:\n        filenames = batch[\"filenames\"]\n        inputs = batch[\"spectrograms\"].to(cfg.device).float()\n\n        for i, fname in enumerate(filenames):\n            if fname.startswith(species):\n                all_matches.append((inputs[i], fname))\n\n        if len(all_matches) == max_n:\n            break\n\n    if not all_matches:\n        print(f\"[WARNING] No samples found for species '{species}'\")\n        return []\n\n    if mode == \"specific\" and target_filename:\n        for input_tensor, fname in all_matches:\n            if target_filename.split('.')[0] == fname.split(\"_\")[0]:\n                return [(input_tensor, fname)]\n        print(f\"[WARNING] Specified file '{target_filename}' not found for species '{species}'\")\n        return []\n\n    elif mode == \"consecutive\":\n        return all_matches[:num_samples]\n\n    elif mode == \"random\":\n        return random.sample(all_matches, min(len(all_matches), num_samples))\n\n    else:\n        raise ValueError(f\"[ERROR] Unknown mode: {mode}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:33:43.858598Z","iopub.execute_input":"2025-06-10T16:33:43.859353Z","iopub.status.idle":"2025-06-10T16:33:43.866785Z","shell.execute_reply.started":"2025-06-10T16:33:43.859327Z","shell.execute_reply":"2025-06-10T16:33:43.866058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_data(\n    df, \n    collection=None, \n    rating=None, \n    primary_label=None, \n    common_name=None, \n    filename=None,\n    random_sample=True\n):\n    # Step 1: Filter the dataframe\n    filtered = df.copy()\n    \n    if collection:\n        filtered = filtered[filtered['collection'] == collection]\n    if rating is not None:\n        filtered = filtered[filtered['rating'] == rating]\n    if primary_label:\n        filtered = filtered[filtered['primary_label'] == primary_label]\n    if filename:\n        filtered = filtered[filtered['filename'] == filename]\n    if common_name:\n        filtered = filtered[filtered['common_name'].str.contains(common_name, case=False, na=False)]\n\n    # Step 2: Select sample\n    if filtered.empty:\n        print(\"No matches found with the given filters.\")\n        return\n\n    return filtered  # return metadata for inspection\n\n\ndef play_audio(audio_dir, row):\n    #sample = filtered.sample(1) if random_sample else filtered.iloc[[0]]\n    #row = sample.iloc[0]\n    \n    # Step 3: Load and play\n    path = os.path.join(audio_dir, row['filename'])\n    print(f\"â–¶ï¸ Playing: {row['common_name']} [{row['primary_label']}]\")\n    print(f\"Collection: {row['collection']}, Rating: {row['rating']}\")\n    print(f\"File: {row['filename']}\")\n    display(Audio(path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:00:48.606606Z","iopub.execute_input":"2025-06-10T16:00:48.607321Z","iopub.status.idle":"2025-06-10T16:00:48.613605Z","shell.execute_reply.started":"2025-06-10T16:00:48.607296Z","shell.execute_reply":"2025-06-10T16:00:48.612770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GradCam and Attention","metadata":{}},{"cell_type":"code","source":"class GradCAM:\n    def __init__(self, model, target_layer):\n        \"\"\"\n        Initializes GradCAM for a specified model and layer.\n        \"\"\"\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        \n        # Register hooks: Check if full backward is allowed\n        try:\n            self.hook = target_layer.register_full_backward_hook(self.save_gradients)\n            if cfg.debug:\n                print(\"[INFO] Full Backward Hook Registered Successfully.\")\n        except RuntimeError:\n            # Fallback if full hook is not available\n            self.hook = target_layer.register_backward_hook(self.save_gradients)\n            print(\"[WARNING] Full Backward Hook not available, using regular backward hook.\")\n\n        target_layer.register_forward_hook(self.save_activations)\n\n    def save_activations(self, module, input, output):\n        \"\"\" Save activations from the forward pass. \"\"\"\n        self.activations = output\n\n    def save_gradients(self, module, grad_input, grad_output):\n        \"\"\" Save gradients from the backward pass. \"\"\"\n        self.gradients = grad_output[0]\n\n    def __call__(self, inputs, class_idx=None):\n        \"\"\"\n        Generate Grad-CAM heatmap for the given input.\n        \"\"\"\n        self.model.zero_grad()\n        outputs = self.model(inputs)\n\n        if class_idx is None:\n            class_idx = outputs.argmax(dim=1)\n\n        one_hot = torch.zeros_like(outputs)\n        for i, idx in enumerate(class_idx):\n            one_hot[i, idx] = 1\n        outputs.backward(gradient=one_hot, retain_graph=True)\n\n        # Compute Grad-CAM\n        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n        activations = self.activations[0]\n        \n        for i in range(len(pooled_gradients)):\n            activations[i, :, :] *= pooled_gradients[i]\n\n        heatmap = torch.mean(activations, dim=0).cpu().detach().numpy()\n        heatmap = np.maximum(heatmap, 0)  # ReLU\n        heatmap /= np.max(heatmap)  # Normalize\n        return heatmap\n\n    def visualize(self, spec_path, heatmap, filename):\n        \"\"\"\n        Visualizes the spectrogram with Grad-CAM overlay, side by side.\n        \"\"\"\n        spec = np.load(spec_path)\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n\n        # Original Spectrogram\n        ax1.imshow(spec, aspect='auto', origin='lower', cmap='viridis')\n        ax1.set_title(f\"Original Spectrogram: {filename}\")\n        ax1.set_xlabel(\"Time Frames\")\n        ax1.set_ylabel(\"Mel Bands\")\n\n        # Grad-CAM Overlay\n        ax2.imshow(np.flipud(spec), aspect='auto', origin='lower', cmap='viridis')\n        ax2.imshow(cv2.resize(heatmap, (spec.shape[1], spec.shape[0])), alpha=0.4, cmap='jet')\n        ax2.set_title(f\"Grad-CAM Visualization: {filename}\")\n        ax2.set_xlabel(\"Time Frames\")\n        ax2.set_ylabel(\"Mel Bands\")\n\n        plt.tight_layout()\n        plt.show()\n\ndef visualize_grad_cam_samples(loader, grad_cam, num_samples=5):\n    \"\"\"\n    Visualizes a limited number of Grad-CAM samples from the loader.\n    \"\"\"\n    count = 0\n    for batch in tqdm(loader, desc=\"Visualizing Grad-CAM\"):\n        inputs = batch[\"spectrograms\"].to(cfg.device).float()\n        filenames = batch[\"filenames\"]\n\n        # Forward pass to get heatmaps\n        for i, input_tensor in enumerate(inputs):\n            if count >= num_samples:\n                return  # Limit reached\n\n            # Compute Grad-CAM\n            heatmap = grad_cam(input_tensor.unsqueeze(0))\n\n            # Load the original spectrogram\n            spec_path = os.path.join(cfg.spectrogram_dir, \"precomputed_spectrograms\", filenames[i])\n\n            # Visualize\n            grad_cam.visualize(spec_path, heatmap, filenames[i])\n            count += 1\n\ndef visualize_grad_cam_by_species(loader, grad_cam, species, num_samples=5):\n    \"\"\"\n    Visualizes Grad-CAM for random samples matching the specified species.\n\n    Parameters:\n    ----------\n    loader : DataLoader\n        The DataLoader providing batches with keys: spectrograms, labels, filenames.\n    grad_cam : GradCAM\n        The initialized GradCAM object.\n    species : str\n        The target species (matches first part of filename, e.g., '1139490').\n    num_samples : int\n        Number of random Grad-CAM visualizations to generate.\n    \"\"\"\n    matching_samples = []\n\n    # First pass: Collect all matching samples\n    for batch in tqdm(loader, desc=f\"Searching for species {species}\"):\n        filenames = batch[\"filenames\"]\n        inputs = batch[\"spectrograms\"]\n\n        for i, fname in enumerate(filenames):\n            if fname.startswith(species):\n                matching_samples.append((inputs[i], fname))\n\n    if len(matching_samples) == 0:\n        print(f\"[WARNING] No samples found for species '{species}'\")\n        return\n\n    # Randomly select up to num_samples\n    selected_samples = random.sample(matching_samples, min(num_samples, len(matching_samples)))\n\n    for input_tensor, fname in selected_samples:\n        input_tensor = input_tensor.unsqueeze(0).to(cfg.device).float()\n        heatmap = grad_cam(input_tensor)\n\n        spec_path = os.path.join(cfg.spectrogram_dir, \"precomputed_spectograms\", fname)\n        grad_cam.visualize(spec_path, heatmap, fname)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:36:46.002356Z","iopub.execute_input":"2025-06-10T16:36:46.002621Z","iopub.status.idle":"2025-06-10T16:36:46.017380Z","shell.execute_reply.started":"2025-06-10T16:36:46.002602Z","shell.execute_reply":"2025-06-10T16:36:46.016586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttentionVisualizer:\n    def __init__(self, model):\n        self.model = model\n\n    def get_temporal_attention(self):\n        \"\"\"\n        Returns the averaged temporal attention across channels.\n\n        Assumes model.attention_weights shape: [B, C, 1, T]\n        Returns:\n            np.ndarray of shape [B, T] â†’ temporal attention per time step\n        \"\"\"\n        attn = self.model.attention_weights  # [B, C, 1, T]\n        attn = attn.mean(dim=1)              # â†’ [B, 1, T]\n        attn = attn.squeeze(1)               # â†’ [B, T]\n        return attn.cpu().numpy()\n\n    def get_frequency_attention(self):\n        \"\"\"\n        Returns the averaged frequency attention across channels.\n\n        Assumes model.attention_weights shape: [B, C, F, 1]\n        Returns:\n            np.ndarray of shape [B, T] â†’ temporal attention per time step\n        \"\"\"\n        attn = self.model.attention_weights  # [B, C, F. 1]\n        attn = attn.mean(dim=1)              # â†’ [B, F, 1]\n        attn = attn.squeeze(2)               # â†’ [B, F]\n        return attn.cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:46:40.361099Z","iopub.execute_input":"2025-06-10T16:46:40.361851Z","iopub.status.idle":"2025-06-10T16:46:40.366950Z","shell.execute_reply.started":"2025-06-10T16:46:40.361827Z","shell.execute_reply":"2025-06-10T16:46:40.366184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_multiple_gradcams_by_layer(selected_sample, model, gradcam_class, cols=4):\n    \"\"\"\n    Visualizes Grad-CAM overlays from multiple layers on spectrograms.\n    \n    Parameters:\n    -----------\n    selected_sample : list of (tensor, str)\n        List of (input_tensor, filename) tuples.\n    model : torch.nn.Module\n        The trained model.\n    layers : list of torch.nn.Module\n        List of layers to hook for Grad-CAM.\n    layer_names : list of str\n        Names for display corresponding to each layer.\n    gradcam_class : type\n        GradCAM class (used to instantiate per layer).\n    \"\"\"\n    # Select layers\n    layers_to_use = [\n        model.backbone.blocks[0],\n        model.backbone.blocks[1],\n        model.backbone.blocks[2],\n        model.backbone.blocks[3],\n        model.backbone.blocks[4],\n        model.backbone.blocks[5],\n        model.backbone.blocks[6],\n    ]\n    layer_names = [f\"Block {i}\" for i in range(len(layers_to_use))]\n\n    input_tensor, filename = selected_sample\n    spec_path = os.path.join(cfg.spectrogram_dir, \"precomputed_spectrograms\", filename)\n    spec = np.load(spec_path)\n    \n\n    total = len(layers_to_use) + 1  # +1 for original\n    rows = (total + cols - 1) // cols\n\n    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 4.5 * rows))\n    axes = axes.flatten()\n\n    # Original\n    axes[0].imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n    axes[0].set_title(f\"Original Spectrogram\\n{filename}\")\n    axes[0].set_xlabel(\"Time Frames\")\n    axes[0].set_ylabel(\"Mel Bands\")\n\n    for i, (layer, name) in enumerate(zip(layers_to_use, layer_names)):\n        grad_cam = gradcam_class(model, layer)\n        heatmap = grad_cam(input_tensor.unsqueeze(0))  # shape: [H, W]\n\n        axes[i + 1].imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        im = axes[i + 1].imshow(cv2.resize(heatmap, (spec.shape[1], spec.shape[0])), alpha=0.4, cmap='jet', origin='lower')\n        axes[i + 1].set_title(f\"Grad-CAM: {name}\")\n        axes[i + 1].set_xlabel(\"Time Frames\")\n        axes[i + 1].set_ylabel(\"Mel Bands\")\n\n        fig.colorbar(im, ax=axes[i+1], fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.785124Z","iopub.execute_input":"2025-06-10T14:59:09.785420Z","iopub.status.idle":"2025-06-10T14:59:09.802385Z","shell.execute_reply.started":"2025-06-10T14:59:09.785405Z","shell.execute_reply":"2025-06-10T14:59:09.801683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init(model, conv_layer=-1):\n    target_layer = model.backbone.blocks[conv_layer] \n\n    grad_cam = GradCAM(model, target_layer)\n    attention_visualizer = AttentionVisualizer(model)\n\n    return grad_cam, attention_visualizer\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T14:59:09.802956Z","iopub.execute_input":"2025-06-10T14:59:09.803134Z","iopub.status.idle":"2025-06-10T14:59:09.817881Z","shell.execute_reply.started":"2025-06-10T14:59:09.803121Z","shell.execute_reply":"2025-06-10T14:59:09.817362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_gradcam_and_time_wise_attention(selected_samples, grad_cam, attention_visualizer, n=10):\n    \"\"\"\n    Visualizes Grad-CAM and Attention Maps for random samples of a given species.\n    \"\"\"\n    \n    for input_tensor, filename in selected_samples[:n]:\n        input_tensor = input_tensor.unsqueeze(0).to(cfg.device)\n        spec_path = os.path.join(cfg.spectrogram_dir, \"precomputed_spectrograms\", filename)\n        spec = np.load(spec_path)\n\n        # Compute GradCAM\n        heatmap = grad_cam(input_tensor)\n        cam = cv2.resize(heatmap, (spec.shape[1], spec.shape[0]))\n\n        # Trigger Attention Forward Pass\n        with torch.no_grad():\n            _ = attention_visualizer.model(input_tensor)\n\n        # Get Attention Map â†’ collapse over channels\n        attn_raw = attention_visualizer.get_temporal_attention()[0]  # [T,]\n        attn_raw = np.maximum(attn_raw, 0)\n        attn_raw /= attn_raw.max() if attn_raw.max() > 0 else 1\n\n        # Create vertical attention heatmap (each value stretched over spectrogram height)\n        framewise_preds = model_time.get_framewise_output(input_tensor).squeeze(0).cpu().detach().numpy()  # [num_classes, T]\n        attention_heatmap = np.tile(attn_raw, (spec.shape[0], 1))  # [Freq, Time]\n        \n        # Combined Plot\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 5))\n\n        # Original\n        ax1.imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        ax1.set_title(f\"Original Spectrogram\\nFile: {filename}\")\n        ax1.set_xlabel(\"Time Frames\")\n        ax1.set_ylabel(\"Mel Bands\")\n\n        # GradCAM\n        ax2.imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        ax2.imshow(cam, aspect='auto', alpha=0.5, cmap='jet', origin='lower')\n        ax2.set_title(\"Grad-CAM Overlay\\nWhich regions most contributed to THIS class decision?\")\n        ax2.set_xlabel(\"Time Frames\")\n        ax2.set_ylabel(\"Mel Bands\")\n\n        # Attention Overlay with vertical heat strips\n        extent = [0, 5, 0, spec.shape[0]]  # [xmin, xmax, ymin, ymax]\n        ax3.imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        ax3.imshow(attention_heatmap, aspect='auto', alpha=0.4, cmap='hot', origin='lower', extent=extent)\n        ax3.set_title(\"Temporal Attention Strips\\nWhere is the model focusing internally?\")\n        ax3.set_xlabel(\"Time Frames\")\n        ax3.set_ylabel(\"Mel Bands\")\n        \n        plt.tight_layout()\n        plt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:28:14.693802Z","iopub.execute_input":"2025-06-10T18:28:14.694094Z","iopub.status.idle":"2025-06-10T18:28:14.703234Z","shell.execute_reply.started":"2025-06-10T18:28:14.694073Z","shell.execute_reply":"2025-06-10T18:28:14.702458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_gradcam_and_frequency_wise_attention(selected_samples, grad_cam, attention_visualizer, n=10):\n    \"\"\"\n    Visualizes Grad-CAM and frequency-wise Attention Maps for selected samples.\n\n    Parameters:\n    -----------\n    selected_samples : list of (input_tensor, filename)\n        Spectrogram tensors and their associated filenames.\n    grad_cam : GradCAM object\n        Initialized with the correct target layer.\n    attention_visualizer : AttentionVisualizer object\n        Must wrap a model that stores frequency attention weights in .get_temporal_attention().\n    n : int\n        Number of samples to visualize.\n    \"\"\"\n\n    for input_tensor, filename in selected_samples[:n]:\n        input_tensor = input_tensor.unsqueeze(0).to(cfg.device)\n        spec_path = os.path.join(cfg.spectrogram_dir, \"precomputed_spectrograms\", filename)\n        spec = np.load(spec_path)\n\n        # === Grad-CAM ===\n        heatmap = grad_cam(input_tensor)  # shape: [H, W]\n        cam = cv2.resize(heatmap, (spec.shape[1], spec.shape[0]))\n\n        # === Trigger forward pass to capture attention ===\n        with torch.no_grad():\n            _ = attention_visualizer.model(input_tensor)\n\n        # === Extract frequency-wise attention weights ===\n        attn_raw = attention_visualizer.get_frequency_attention()[0]  # shape: [Freq,]\n        attn_raw = np.maximum(attn_raw, 0)\n        attn_raw /= attn_raw.max() if attn_raw.max() > 0 else 1\n\n        # Expand attention vertically across time (horizontal lines)\n        attention_heatmap = np.tile(attn_raw[:, np.newaxis], (1, spec.shape[1]))  # [Freq, Time]\n\n        # === Plot ===\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 5))\n\n        # Original Spectrogram\n        ax1.imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        ax1.set_title(f\"Original Spectrogram\\nFile: {filename}\")\n        ax1.set_xlabel(\"Time Frames\")\n        ax1.set_ylabel(\"Mel Bands\")\n\n        # GradCAM\n        ax2.imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        ax2.imshow(cam, aspect='auto', alpha=0.4, cmap='jet', origin='lower')\n        ax2.set_title(\"Grad-CAM Overlay\\nImportant Regions for Prediction\")\n        ax2.set_xlabel(\"Time Frames\")\n        ax2.set_ylabel(\"Mel Bands\")\n\n        # Frequency Attention Overlay\n        ax3.imshow(spec, aspect='auto', cmap='viridis', origin='lower')\n        ax3.imshow(attention_heatmap, aspect='auto', alpha=0.5, cmap='hot', origin='lower')\n        ax3.set_title(\"Frequency-wise Attention\\nWhere is the model focusing?\")\n        ax3.set_xlabel(\"Time Frames\")\n        ax3.set_ylabel(\"Mel Bands\")\n\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:21:30.863887Z","iopub.execute_input":"2025-06-10T18:21:30.864484Z","iopub.status.idle":"2025-06-10T18:21:30.872865Z","shell.execute_reply.started":"2025-06-10T18:21:30.864460Z","shell.execute_reply":"2025-06-10T18:21:30.872040Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predictions against spectrogram","metadata":{}},{"cell_type":"code","source":"def plot_prediction_overview(spec, framewise_preds, output, filename, index_to_label=None, label_to_species=None, top_k=5):\n    \"\"\"\n    Plots the spectrogram and all class framewise predictions (as in the example image).\n\n    Parameters:\n    -----------\n    spec : np.ndarray\n        Spectrogram [mel_bins, time]\n    framewise_preds : np.ndarray\n        Prediction matrix [num_classes, time]\n    filename : str\n        Name for title\n    index_to_label : dict\n        Optional class index â†’ label\n    label_to_species : dict\n        Optional label â†’ species name\n    top_k : int\n        Number of top predictions to print\n    \"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 10), gridspec_kw={'height_ratios': [4, 3, 3]})\n\n    # Spectrogram\n    ax1.imshow(spec, aspect='auto', origin='lower', cmap='viridis')\n    ax1.set_title(filename)\n    ax1.set_ylabel(\"mel bin\")\n    ax1.set_xlabel(\"frame\")\n\n    # Framewise prediction heatmap (x-axis: 0 to 5 sec)\n    n_frames = framewise_preds.shape[1]\n    extent = [0, 5, 0, framewise_preds.shape[0]]  # [xmin, xmax, ymin, ymax]\n    im = ax2.imshow(framewise_preds, aspect='auto', origin='lower', cmap='Blues', extent=extent)\n    ax2.set_title(\"When the sound event is detected\")\n    ax2.set_ylabel(\"species\")\n    ax2.set_xlabel(\"frame / sec\")\n\n    # Add colorbar WITHOUT shrinking the plot\n    divider2 = make_axes_locatable(ax2)\n    cax2 = divider2.append_axes(\"right\", size=\"2.5%\", pad=0.05)\n    plt.colorbar(im, cax=cax2)\n    #fig.colorbar(im, ax=ax2)\n\n    # Clip-level predictions\n    im3 = ax3.imshow(output, aspect='auto', origin='lower', cmap='Greens', vmin=0.0, vmax=1)\n    ax3.set_title(f\"Confidence in the detection per species\")\n    ax3.set_ylabel(\"species\")\n    ax3.set_xlabel(\"prediction\")\n    #fig.colorbar(im3, ax=ax3)\n\n    # Add colorbar WITHOUT shrinking the plot\n    divider3 = make_axes_locatable(ax3)\n    cax3 = divider3.append_axes(\"right\", size=\"2.5%\", pad=0.05)\n    plt.colorbar(im3, cax=cax3)\n\n    # Optional: print top_k predictions\n    #mean_scores = framewise_preds.mean(axis=1) # (206, )\n    #top_classes = framewise_preds.argsort()[::-1]#[:top_k] # (206, 10)\n    print(\"\\nTop Predictions:\")\n    top_classes = output.flatten().argsort()[::-1][:top_k]\n    for i, idx in enumerate(top_classes):\n        label = index_to_label.get(idx, f\"Class {idx}\") if index_to_label else f\"Class {idx}\"\n        species = label_to_species.get(label, \"\") if label_to_species else \"\"\n        print(f\"{i+1}. {label:<7} | Class index: {idx:<3} | Score: {output.flatten()[idx]:.3f} | {species}\")\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_species_predictions(model, samples, n=10):\n    for input_tensor, filename in samples[:n]:\n        input_tensor = input_tensor.unsqueeze(0).to(cfg.device)\n        \n        spec_path = os.path.join(cfg.spectrogram_dir, \"precomputed_spectrograms\", filename)\n        spec_np = np.load(spec_path)  # [mel_bins, time]\n    \n        # Run model + get framewise predictions\n        framewise = model.get_framewise_output(input_tensor).squeeze(0).cpu().detach().numpy()  # [num_classes, T]\n        \n        outputs = model(input_tensor.float())\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy().transpose(1, 0) # 206, 1\n        plot_prediction_overview(\n            spec=spec_np,\n            framewise_preds=framewise,\n            output=outputs,\n            filename=filename,\n            index_to_label=index_to_label,\n            label_to_species=label_to_class,\n            top_k=2,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:14:57.530918Z","iopub.execute_input":"2025-06-10T18:14:57.531154Z","iopub.status.idle":"2025-06-10T18:14:57.546970Z","shell.execute_reply.started":"2025-06-10T18:14:57.531138Z","shell.execute_reply":"2025-06-10T18:14:57.546220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analysis and performance metrics","metadata":{}},{"cell_type":"code","source":"# === Load model ===\nmodel_time = EfficientNetTimeSED(cfg).to(cfg.device)\nprint(f\"[INFO] Loading weights from {cfg.timewise_weights_path}\")\nraw_state = torch.load(cfg.timewise_weights_path, map_location=cfg.device)\nremapped_state = remap_attention_keys(raw_state)\nmodel_time.load_state_dict(remapped_state, strict=False)\nmodel_time.eval()\n\nmodel_freq = EfficientNetFrequencySED(cfg).to(cfg.device)\nprint(f\"[INFO] Loading weights from {cfg.freqwise_weights_path}\")\nraw_state = torch.load(cfg.freqwise_weights_path, map_location=cfg.device)\nmodel_freq.load_state_dict(raw_state, strict=False)\nmodel_freq.eval()\n\n# === Load mappings ===\nlabel_to_class, label_to_index, index_to_label, _ = get_mappings()\nnum_classes = len(label_to_index)\n\n# === Prepare dataset and loader ===\nmetadata = pd.read_csv(cfg.spectrograms_metadata_path)\ndataset = PrecomputedSpectrogramDataset(metadata, cfg.spectrogram_dir)\nloader = create_dataloader(dataset, cfg, shuffle=False, collate_fn=lambda x: collate_fn(x, mixup=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:37:17.320815Z","iopub.execute_input":"2025-06-10T15:37:17.321663Z","iopub.status.idle":"2025-06-10T15:37:21.118485Z","shell.execute_reply.started":"2025-06-10T15:37:17.321627Z","shell.execute_reply":"2025-06-10T15:37:21.117665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_evaluator = ModelEvaluator(model_time, loader, cfg, label_to_class, index_to_label)\ny_true_t, y_pred_t = time_evaluator.run_inference() # (56168, 206), (56168, 206)\ndf_results_t = time_evaluator.calculate_per_class_auc(y_true_t, y_pred_t)\n\n# === Display table ===\nprint(df_results_t.sort_values(by=\"auc\", ascending=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:42:54.789906Z","iopub.execute_input":"2025-06-10T15:42:54.790124Z","iopub.status.idle":"2025-06-10T15:44:00.354298Z","shell.execute_reply.started":"2025-06-10T15:42:54.790107Z","shell.execute_reply":"2025-06-10T15:44:00.353507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluator = ModelEvaluator(model_freq, loader, cfg, label_to_class, index_to_label)\ny_true, y_pred = evaluator.run_inference() # (56168, 206), (56168, 206)\ndf_results = evaluator.calculate_per_class_auc(y_true, y_pred)\n\n# === Display table ===\nprint(df_results.sort_values(by=\"auc\", ascending=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:41:47.684656Z","iopub.execute_input":"2025-06-10T15:41:47.685117Z","iopub.status.idle":"2025-06-10T15:42:51.904244Z","shell.execute_reply.started":"2025-06-10T15:41:47.685094Z","shell.execute_reply":"2025-06-10T15:42:51.903295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluator.plot_results(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:42:51.912506Z","iopub.execute_input":"2025-06-10T15:42:51.912746Z","iopub.status.idle":"2025-06-10T15:42:54.788372Z","shell.execute_reply.started":"2025-06-10T15:42:51.912707Z","shell.execute_reply":"2025-06-10T15:42:54.787733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's now analyze the predictions of our model.","metadata":{}},{"cell_type":"code","source":"analyzer = ModelAnalyzer(df_results, y_true, y_pred)\nanalyzer.compute_best_thresholds()\n\n# Optional: recompute AUC without filtering\ndf_with_auc = analyzer.compute_auc()\n\nanalyzer.compute_accuracy()\n\n# Summary of best and worst classes\nanalyzer.summarize_best_worst()\n\n# Add precision-recall\ndf_with_pr = analyzer.compute_precision_recall()\n\n# Add confusion stats\ndf_with_confusion = analyzer.compute_confusion_stats()\n\n# Display everything\nanalyzer.display_full_dataframe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:38.128796Z","iopub.execute_input":"2025-06-10T15:53:38.129104Z","iopub.status.idle":"2025-06-10T15:53:48.965630Z","shell.execute_reply.started":"2025-06-10T15:53:38.129084Z","shell.execute_reply":"2025-06-10T15:53:48.964886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analyzer.plot_stats(df_with_confusion, n=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:48.966607Z","iopub.execute_input":"2025-06-10T15:53:48.966790Z","iopub.status.idle":"2025-06-10T15:53:50.024713Z","shell.execute_reply.started":"2025-06-10T15:53:48.966775Z","shell.execute_reply":"2025-06-10T15:53:50.024125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As can be seen, despite the AUC being very high, there are several false positives and negatives, especially from the bird's category. Let's now analyze other metrics, like precision and accuracy.","metadata":{}},{"cell_type":"code","source":"analyzer.plot_performance_metrics()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:55:11.977675Z","iopub.execute_input":"2025-06-10T15:55:11.978430Z","iopub.status.idle":"2025-06-10T15:55:12.074522Z","shell.execute_reply.started":"2025-06-10T15:55:11.978388Z","shell.execute_reply":"2025-06-10T15:55:12.073714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Despite the AUC being high, several species have an extremely low precision and accuracy. Let's analyze the lowest one, `turvul`.","metadata":{}},{"cell_type":"code","source":"# Get details for a specific species\ninfo = analyzer.get_species_info(\"turvul\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:01:40.977265Z","iopub.execute_input":"2025-06-10T15:01:40.977516Z","iopub.status.idle":"2025-06-10T15:01:40.989480Z","shell.execute_reply.started":"2025-06-10T15:01:40.977497Z","shell.execute_reply":"2025-06-10T15:01:40.988737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This bird has precision and accuracy extremely close to 0. Let's analyze way.","metadata":{}},{"cell_type":"code","source":"def print_stats(idx):\n\n    true_class = y_true[:, idx]\n    pred_class = y_pred[:, idx]\n    print(\"Class id\", index_to_label[idx])\n    print(\"Max pred for class 1 samples:\", np.max(pred_class[true_class == 1]))\n    print(\"Min pred for class 1 samples:\", np.min(pred_class[true_class == 1]))\n    print(\"Mean pred for class 1 samples:\", np.mean(pred_class[true_class == 1]))\n    print(\"Max pred for non-class 1 samples:\", np.max(pred_class[true_class == 0]))\n    print(\"Mean pred for non-class 1 samples:\", np.mean(pred_class[true_class == 0]))\n    roc = roc_auc_score(true_class, pred_class)\n    print(\"ROC:\", roc)\n\nprint_stats(idx=182)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:40:43.681588Z","iopub.execute_input":"2025-06-10T17:40:43.682212Z","iopub.status.idle":"2025-06-10T17:40:43.715291Z","shell.execute_reply.started":"2025-06-10T17:40:43.682157Z","shell.execute_reply":"2025-06-10T17:40:43.714530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is extremely confused about this bird. Even the maximum prediction is quite low, with 0.15, while the highest non belonging to this class reach 0.89. The lowest prediction is as low as 0.0001, so it seems like it has no idea what to look for with this bird. Let's see its distribution and listen to some audios.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(cfg.metadata_path)\nsample = filter_data(train_df, primary_label=\"turvul\") # 41778\nsample.head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:01:53.355778Z","iopub.execute_input":"2025-06-10T16:01:53.356509Z","iopub.status.idle":"2025-06-10T16:01:53.458470Z","shell.execute_reply.started":"2025-06-10T16:01:53.356486Z","shell.execute_reply":"2025-06-10T16:01:53.457645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"play_audio(cfg.train_data_path, sample.iloc[1])\nplay_audio(cfg.train_data_path, sample.iloc[3])\nplay_audio(cfg.train_data_path, sample.iloc[4])\nplay_audio(cfg.train_data_path, sample.iloc[5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:04:07.585535Z","iopub.execute_input":"2025-06-10T16:04:07.585810Z","iopub.status.idle":"2025-06-10T16:04:07.654770Z","shell.execute_reply.started":"2025-06-10T16:04:07.585792Z","shell.execute_reply":"2025-06-10T16:04:07.654082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As can be heard, they are extremely different audios. It seems to be quite impossible to recognize the bird, even for a human. Different sounds and a lot of birds in the background that are not flagged as secondary labels. The geographical locatin is different for almost every sample and the author is different too. That's why including metadata would have been probably a good idea.","metadata":{}},{"cell_type":"code","source":"turvul_samples_rufus = select_species_samples(\n    loader,\n    species = \"turvul\",\n    mode = \"specific\",\n    num_samples = 10,\n    target_filename = 'turvul/XC748979.ogg'\n)\nturvul_samples_hiss = select_species_samples(\n    loader,\n    species = \"turvul\",\n    mode = \"specific\",\n    num_samples = 10,\n    target_filename = 'turvul/XC904279.ogg'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:51:31.956884Z","iopub.execute_input":"2025-06-10T16:51:31.957467Z","iopub.status.idle":"2025-06-10T16:53:30.881886Z","shell.execute_reply.started":"2025-06-10T16:51:31.957443Z","shell.execute_reply":"2025-06-10T16:53:30.880891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize\nvisualize_multiple_gradcams_by_layer(turvul_samples_rufus[0], model_freq, GradCAM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:39:48.451663Z","iopub.execute_input":"2025-06-10T16:39:48.451964Z","iopub.status.idle":"2025-06-10T16:39:50.964716Z","shell.execute_reply.started":"2025-06-10T16:39:48.451939Z","shell.execute_reply":"2025-06-10T16:39:50.963507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the spectrogram we can distinguish two different sounds, the one at low frequency and the constant one at higher frequency. It has not figured out yet on which of the two it should focus. Let's inspect now the `hiss` call.","metadata":{}},{"cell_type":"code","source":"# Visualize\nvisualize_multiple_gradcams_by_layer(turvul_samples_hiss[0], model_freq, GradCAM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:53:30.883667Z","iopub.execute_input":"2025-06-10T16:53:30.883921Z","iopub.status.idle":"2025-06-10T16:53:33.338555Z","shell.execute_reply.started":"2025-06-10T16:53:30.883898Z","shell.execute_reply":"2025-06-10T16:53:33.337606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here the hiss is quite clear in the spectrogram, however there is only one such sample and it doesn't seem to be able to recognize it properly. The fifth gradcam focus in that area but the last one randomly focus in higher frequencies and last time frames. ","metadata":{}},{"cell_type":"code","source":"# Set up GradCAM and Attention Visualizer\ngrad_cam, attention_visualizer = init(model_freq, conv_layer=5)\n\nvisualize_gradcam_and_frequency_wise_attention(turvul_samples_hiss, grad_cam, attention_visualizer, n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:15:35.714328Z","iopub.execute_input":"2025-06-10T17:15:35.714934Z","iopub.status.idle":"2025-06-10T17:15:36.706403Z","shell.execute_reply.started":"2025-06-10T17:15:35.714912Z","shell.execute_reply":"2025-06-10T17:15:36.705650Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems that in the second to last convolutional layer the network managed to focus on the hiss, but the same understanding is not retained by the frequency attention model.","metadata":{}},{"cell_type":"code","source":"# Set up GradCAM and Attention Visualizer\ngrad_cam, attention_visualizer = init(model_time, conv_layer=5)\n\n\nvisualize_gradcam_and_time_wise_attention(turvul_samples_hiss, grad_cam, attention_visualizer, n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:28:21.871404Z","iopub.execute_input":"2025-06-10T18:28:21.871682Z","iopub.status.idle":"2025-06-10T18:28:22.834048Z","shell.execute_reply.started":"2025-06-10T18:28:21.871662Z","shell.execute_reply":"2025-06-10T18:28:22.833244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The temporal attention here spans basically the whole 5 seconds, so it's not as useful.","metadata":{}},{"cell_type":"code","source":"plot_species_predictions(model_time, turvul_samples_hiss, n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:30:59.818616Z","iopub.execute_input":"2025-06-10T17:30:59.819138Z","iopub.status.idle":"2025-06-10T17:31:00.513017Z","shell.execute_reply.started":"2025-06-10T17:30:59.819116Z","shell.execute_reply":"2025-06-10T17:31:00.512327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The bluest lines corresponds to when the sound event is detected, which more or less correspond to when the hiss is actually present in the spectrogram.","metadata":{}},{"cell_type":"markdown","source":"Let's now analyze the best predicted bird:","metadata":{}},{"cell_type":"code","source":"# Get details for a specific species\ninfo = analyzer.get_species_info(\"compau\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:50.923523Z","iopub.execute_input":"2025-06-10T17:38:50.924069Z","iopub.status.idle":"2025-06-10T17:38:50.937841Z","shell.execute_reply.started":"2025-06-10T17:38:50.924046Z","shell.execute_reply":"2025-06-10T17:38:50.937212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_stats(idx=97)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:41:07.122538Z","iopub.execute_input":"2025-06-10T17:41:07.123257Z","iopub.status.idle":"2025-06-10T17:41:07.153365Z","shell.execute_reply.started":"2025-06-10T17:41:07.123232Z","shell.execute_reply":"2025-06-10T17:41:07.152633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is much more confident about the preditions on these bird, as the mean prediction of 0.8 shows, but some confusion still persists. There are wrong predictions with .9 of confidence and correct predictions as low as 0.00685, which is not ideal. Let's see why.","metadata":{}},{"cell_type":"code","source":"filtered_compau = filter_data(train_df, primary_label=\"compau\") # 41778\nfiltered_compau.head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:43:47.186047Z","iopub.execute_input":"2025-06-10T17:43:47.186771Z","iopub.status.idle":"2025-06-10T17:43:47.209825Z","shell.execute_reply.started":"2025-06-10T17:43:47.186746Z","shell.execute_reply":"2025-06-10T17:43:47.209270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"play_audio(cfg.train_data_path, filtered_compau.iloc[1])\nplay_audio(cfg.train_data_path, filtered_compau.iloc[3])\nplay_audio(cfg.train_data_path, filtered_compau.iloc[10])\nplay_audio(cfg.train_data_path, filtered_compau.iloc[14])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:48:08.141957Z","iopub.execute_input":"2025-06-10T17:48:08.142251Z","iopub.status.idle":"2025-06-10T17:48:08.189953Z","shell.execute_reply.started":"2025-06-10T17:48:08.142230Z","shell.execute_reply":"2025-06-10T17:48:08.189228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compau_samples_call = select_species_samples(\n    loader,\n    species = \"compau\",\n    mode = \"specific\",\n    num_samples = 10,\n    target_filename = 'compau/XC112631.ogg'\n)\ncompau_samples_flight_call = select_species_samples(\n    loader,\n    species = \"compau\",\n    mode = \"specific\",\n    num_samples = 10,\n    target_filename = 'compau/XC123378.ogg'\n)\ncompau_samples_song = select_species_samples(\n    loader,\n    species = \"compau\",\n    mode = \"specific\",\n    num_samples = 10,\n    target_filename = 'compau/XC121382.ogg'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:48:25.384766Z","iopub.execute_input":"2025-06-10T17:48:25.385030Z","iopub.status.idle":"2025-06-10T17:49:31.831052Z","shell.execute_reply.started":"2025-06-10T17:48:25.385012Z","shell.execute_reply":"2025-06-10T17:49:31.830042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize\nvisualize_multiple_gradcams_by_layer(compau_samples_call[0], model_freq, GradCAM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:52:39.977857Z","iopub.execute_input":"2025-06-10T17:52:39.978412Z","iopub.status.idle":"2025-06-10T17:52:42.308247Z","shell.execute_reply.started":"2025-06-10T17:52:39.978386Z","shell.execute_reply":"2025-06-10T17:52:42.307466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize\nvisualize_multiple_gradcams_by_layer(compau_samples_flight_call[0], model_freq, GradCAM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:52:23.833229Z","iopub.execute_input":"2025-06-10T17:52:23.833757Z","iopub.status.idle":"2025-06-10T17:52:26.249359Z","shell.execute_reply.started":"2025-06-10T17:52:23.833733Z","shell.execute_reply":"2025-06-10T17:52:26.248392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize\nvisualize_multiple_gradcams_by_layer(compau_samples_song[0], model_freq, GradCAM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:52:26.250451Z","iopub.execute_input":"2025-06-10T17:52:26.250717Z","iopub.status.idle":"2025-06-10T17:52:28.726213Z","shell.execute_reply.started":"2025-06-10T17:52:26.250700Z","shell.execute_reply":"2025-06-10T17:52:28.725425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up GradCAM and Attention Visualizer\ngrad_cam, attention_visualizer = init(model_freq, conv_layer=6)\n\nvisualize_gradcam_and_frequency_wise_attention(compau_samples_call, grad_cam, attention_visualizer, n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:21:53.344283Z","iopub.execute_input":"2025-06-10T18:21:53.344564Z","iopub.status.idle":"2025-06-10T18:21:54.249228Z","shell.execute_reply.started":"2025-06-10T18:21:53.344548Z","shell.execute_reply":"2025-06-10T18:21:54.248488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The frequency attention model doesn't seem to have a clear idea of what to look for. The frequency bands where the call lies holds a high value in the attenton heatmap, but not as high as the highest frequency apparently. The model is overfitting the training dataset.","metadata":{}},{"cell_type":"code","source":"# Set up GradCAM and Attention Visualizer\ngrad_cam, attention_visualizer = init(model_time, conv_layer=5)\n\nvisualize_gradcam_and_time_wise_attention(compau_samples_call, grad_cam, attention_visualizer, n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:28:30.452542Z","iopub.execute_input":"2025-06-10T18:28:30.453077Z","iopub.status.idle":"2025-06-10T18:28:31.347400Z","shell.execute_reply.started":"2025-06-10T18:28:30.453053Z","shell.execute_reply":"2025-06-10T18:28:31.346724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The temporal model seems to have a better idea of where the sound even happen, even if the confusion persists. The two sounds location in time is mainly recognized by the model.","metadata":{}},{"cell_type":"code","source":"plot_species_predictions(model_time, compau_samples_call, n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:15:42.213284Z","iopub.execute_input":"2025-06-10T18:15:42.213578Z","iopub.status.idle":"2025-06-10T18:15:42.921680Z","shell.execute_reply.started":"2025-06-10T18:15:42.213558Z","shell.execute_reply":"2025-06-10T18:15:42.921028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The confidence in the prediction for `compau` is quite high. In the second graph, it can be clearly seen that the species with index 97 has the better predicted sound event locations, which coincides with the `compau` predictions, of course.","metadata":{}},{"cell_type":"markdown","source":"## Chunks Tracker","metadata":{}},{"cell_type":"code","source":"class ChunkStatsTracker:\n    \"\"\"\n    Tracks per-species and per-collection chunk statistics from precomputed files.\n    \"\"\"\n\n    def __init__(self, spectrogram_dir):\n        self.spectrogram_dir = spectrogram_dir\n        self.class_chunk_counts = defaultdict(int)\n        self.collection_counts = defaultdict(int)\n        self.chunk_metadata = []\n\n    def scan_precomputed_chunks(self):\n        \"\"\"\n        Walks through precomputed_spectograms/ and collects species and collection stats.\n        \"\"\"\n        print(f\"[INFO] Scanning directory: {self.spectrogram_dir}\")\n\n        for species_id in os.listdir(self.spectrogram_dir):\n            species_dir = os.path.join(self.spectrogram_dir, species_id)\n            if not os.path.isdir(species_dir):\n                continue\n\n            for file in os.listdir(species_dir):\n                if file.endswith(\".npy\"):\n                    collection = file.split(\"_\")[0]  # assumes e.g., CSA_chunk_0.npy â†’ CSA\n                    self.class_chunk_counts[species_id] += 1\n                    self.collection_counts[collection] += 1\n                    self.chunk_metadata.append({\n                        \"species\": species_id,\n                        \"collection\": collection,\n                        \"filename\": file\n                    })\n\n        print(f\"[INFO] Completed scan: {len(self.chunk_metadata)} chunks found.\")\n\n    def get_summary_dataframe(self):\n        return pd.DataFrame(self.chunk_metadata)\n\n    def plot_species_distribution(self):\n        df = self.get_summary_dataframe()\n        count_df = df.groupby(\"species\").size().reset_index(name=\"chunk_count\")\n        count_df = count_df.sort_values(\"chunk_count\", ascending=False)\n\n        plt.figure(figsize=(16, 6))\n        sns.barplot(x=\"species\", y=\"chunk_count\", data=count_df)\n        plt.xticks(rotation=90)\n        plt.title(\"Number of Chunks per Species\")\n        plt.xlabel(\"Species ID\")\n        plt.ylabel(\"Chunk Count\")\n        plt.tight_layout()\n        plt.show()\n\n    def plot_collection_distribution(self):\n        df = self.get_summary_dataframe()\n        count_df = df.groupby(\"collection\").size().reset_index(name=\"chunk_count\")\n        plt.figure(figsize=(6, 4))\n        sns.barplot(x=\"collection\", y=\"chunk_count\", data=count_df)\n        plt.title(\"Number of Chunks per Collection\")\n        plt.xlabel(\"Collection\")\n        plt.ylabel(\"Chunk Count\")\n        plt.tight_layout()\n        plt.show()\n\n    def save_to_csv(self, path):\n        df = self.get_summary_dataframe()\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        df.to_csv(path, index=False)\n        print(f\"[INFO] Chunk summary saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:00:04.437870Z","iopub.execute_input":"2025-06-10T18:00:04.438709Z","iopub.status.idle":"2025-06-10T18:00:04.448796Z","shell.execute_reply.started":"2025-06-10T18:00:04.438681Z","shell.execute_reply":"2025-06-10T18:00:04.448129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spect_dir = os.path.join(\"/kaggle/input/eda-birdclef2025\", \"precomputed_spectograms\")\ntracker = ChunkStatsTracker(spect_dir)\ntracker.scan_precomputed_chunks()\ntracker.plot_species_distribution()\ntracker.plot_collection_distribution()\ntracker.save_to_csv(os.path.join(cfg.OUTPUT_DIR, \"chunk_summary_0.csv\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:00:05.885833Z","iopub.execute_input":"2025-06-10T18:00:05.886418Z","iopub.status.idle":"2025-06-10T18:04:20.112405Z","shell.execute_reply.started":"2025-06-10T18:00:05.886395Z","shell.execute_reply":"2025-06-10T18:04:20.111732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spect_dir = os.path.join(\"/kaggle/input/precomputing-spectrograms\", \"precomputed_spectrograms\")\n# spect_dir = os.path.join(\"/kaggle/input/eda-birdclef2025\", \"precomputed_spectograms\")\ntracker = ChunkStatsTracker(spect_dir)\ntracker.scan_precomputed_chunks()\ntracker.plot_species_distribution()\ntracker.plot_collection_distribution()\ntracker.save_to_csv(os.path.join(cfg.OUTPUT_DIR, \"chunk_summary_1.csv\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:04:20.113517Z","iopub.execute_input":"2025-06-10T18:04:20.113760Z","iopub.status.idle":"2025-06-10T18:08:14.934184Z","shell.execute_reply.started":"2025-06-10T18:04:20.113744Z","shell.execute_reply":"2025-06-10T18:08:14.933511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spect_dir = os.path.join(\"/kaggle/input/precomputing-spectrograms2\", \"precomputed_spectrograms\")\n# spect_dir = os.path.join(\"/kaggle/input/eda-birdclef2025\", \"precomputed_spectograms\")\ntracker = ChunkStatsTracker(spect_dir)\ntracker.scan_precomputed_chunks()\ntracker.plot_species_distribution()\ntracker.plot_collection_distribution()\ntracker.save_to_csv(os.path.join(cfg.OUTPUT_DIR, \"chunk_summary_2.csv\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:08:14.934905Z","iopub.execute_input":"2025-06-10T18:08:14.935191Z","iopub.status.idle":"2025-06-10T18:11:05.630847Z","shell.execute_reply.started":"2025-06-10T18:08:14.935139Z","shell.execute_reply":"2025-06-10T18:11:05.629893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spect_dir = os.path.join(\"/kaggle/input/precomputing-spectrograms3\", \"precomputed_spectrograms\")\n# spect_dir = os.path.join(\"/kaggle/input/eda-birdclef2025\", \"precomputed_spectograms\")\ntracker = ChunkStatsTracker(spect_dir)\ntracker.scan_precomputed_chunks()\ntracker.plot_species_distribution()\ntracker.plot_collection_distribution()\ntracker.save_to_csv(os.path.join(cfg.OUTPUT_DIR, \"chunk_summary_3.csv\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:11:05.632595Z","iopub.execute_input":"2025-06-10T18:11:05.632824Z","iopub.status.idle":"2025-06-10T18:14:57.519681Z","shell.execute_reply.started":"2025-06-10T18:11:05.632805Z","shell.execute_reply":"2025-06-10T18:14:57.519036Z"}},"outputs":[],"execution_count":null}]}