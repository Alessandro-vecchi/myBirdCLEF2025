{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"},{"sourceId":11875127,"sourceType":"datasetVersion","datasetId":7463065},{"sourceId":239793644,"sourceType":"kernelVersion"},{"sourceId":240246005,"sourceType":"kernelVersion"},{"sourceId":240908835,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n\n# --- Install offline packages ---\ntry:\n    import ace_tools_open\nexcept ModuleNotFoundError:\n    print('Installing ace tools...')\n    !pip install -q /kaggle/input/offline-packages/itables-2.3.0-py3-none-any.whl\n    !pip install -q /kaggle/input/offline-packages/ace_tools_open-0.1.0-py3-none-any.whl\n\ntry:\n    import timm\nexcept ModuleNotFoundError:\n    print('Installing timm...')\n    !pip install -q /kaggle/input/offline-packages/timm-1.0.15-py3-none-any.whl\n    \ntry:\n    import warmup_scheduler\nexcept ModuleNotFoundError:\n    print('Installing warmup-scheduler...')\n    !pip install -q /kaggle/input/offline-packages/warmup_scheduler-0.3.tar.gz\n\n# --- Core libraries ---\nimport os\nimport math\nimport random\nimport time\nimport numpy as np\n\n# --- Data handling ---\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\nfrom dataclasses import dataclass, field\nfrom sklearn.preprocessing import label_binarize\n\n# --- PyTorch ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchaudio.functional import bandpass_biquad\nfrom warmup_scheduler import GradualWarmupScheduler\n\n# --- Audio processing ---\nimport torchaudio\nimport torchaudio.transforms as T\n\n# --- Visualization ---\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Audio, display\nfrom tqdm.notebook import tqdm\nimport ace_tools_open as tools\nimport torchvision\nfrom torchvision.ops.focal_loss import sigmoid_focal_loss\nimport cv2\n\n# --- Parallel and Custom Tools ---\nfrom joblib import Parallel, delayed\nfrom torch.amp import GradScaler, autocast\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom typing import Optional, List, Tuple\nimport timm\nimport tempfile\nimport gc\nimport itertools\nfrom glob import glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:58:53.483411Z","iopub.execute_input":"2025-05-21T11:58:53.483667Z","iopub.status.idle":"2025-05-21T11:59:18.473208Z","shell.execute_reply.started":"2025-05-21T11:58:53.483646Z","shell.execute_reply":"2025-05-21T11:59:18.472433Z"}},"outputs":[{"name":"stdout","text":"Installing ace tools...\nInstalling warmup-scheduler...\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for warmup_scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\nCPU times: user 8.04 s, sys: 1.77 s, total: 9.81 s\nWall time: 25 s\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!find /kaggle/working/pseudo_spectrograms -type f -name \"*.npy\" | wc -l  # 223698","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:04:15.703252Z","iopub.execute_input":"2025-05-21T12:04:15.703490Z","iopub.status.idle":"2025-05-21T12:04:16.171453Z","shell.execute_reply.started":"2025-05-21T12:04:15.703462Z","shell.execute_reply":"2025-05-21T12:04:16.170705Z"}},"outputs":[{"name":"stdout","text":"223698\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"@dataclass\nclass CFG:\n    # General\n    LOAD_DATA: bool = True\n    seed: int = 69\n    debug: bool = False\n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    ## Data paths ##\n    OUTPUT_DIR: str = '/kaggle/working/'\n    temporary_dir: str = field(init=False)\n    real_spectrogram_dir: str = \"/kaggle/input/eda-birdclef2025\" #\"/kaggle/working/precomputed_spectrograms\"\n    real_spectrogram_csv_filename: str = \"spec_metadata.csv\"\n    real_spectrograms_metadata_path: str = field(init=False) # \"filename\", \"primary_label\")\n    pseudo_spectrograms_dir: str = '/kaggle/working/' # \"/kaggle/input/pseudo-labeling-birdclef2025\"\n    pseudo_spectrogram_csv_filename: str = \"pseudo_metadata.csv\"\n    pseudo_spectrograms_metadata_path: str = field(init=False) # \"filename\", \"primary_label\"\n    \n    # Base path to dataset\n    data_path: str = '/kaggle/input/birdclef-2025/'\n    # Key file paths\n    metadata_path: str = field(init=False)\n    taxonomy_path: str = field(init=False)\n    sample_submission_path: str = field(init=False)\n    location_path: str = field(init=False)\n    # Audio data directories\n    train_data_path: str = field(init=False)\n    test_soundscapes_path: str = field(init=False)\n    unlabeled_soundscapes_path: str = field(init=False)\n\n    # Audio config\n    topDB: int = 80\n    FS: int = 32000\n    CHUNK_LENGTH: float = 5.0   # seconds\n    N_FFT: int = 1024\n    HOP_LENGTH: int = 512\n    N_MELS: int = 128\n    FMIN: int = 50\n    FMAX: int = 16000\n    POWER: int = 2\n    SPEC_DTYPE: str = 'float16'  # for disk saving\n    spectrogram_time_frames: int = field(init=False)\n    \n    # VAD and Filtering\n    VAD_ENABLED: bool = False  # Whether to apply Voice Activity Detection\n    VAD_THRESHOLD: float = 0.4  # Confidence threshold for Silero VAD\n    BANDPASS_LOW: int = 300\n    BANDPASS_HIGH: int = 16000\n    VISUALIZE_SKIPPED: bool = False  # Set to True to see spectrograms of skipped chunks\n\n\n    # Training\n    BATCH_SIZE: int = 32\n    EPOCHS: int = 20\n    criterion: str = 'BCEWithLogitsLoss'\n    optimizer: str = 'AdamW'\n    LEARNING_RATE: float = 1e-3\n    weight_decay: float = 1e-5\n    scheduler: str = 'CosineAnnealingLR'\n    min_lr: float = 1e-6\n    n_fold: int = 5\n    num_workers: int = 4\n\n    # Augmentation\n    augment = True\n    aug_prob: float = 0.5\n    mixup_alpha: float = 0.4\n\n    # Model\n    model_name: str = \"efficientnet_b0\" # 'efficientnet_b3_pruned', 'efficientnetv2_rw_m', 'efficientvit_l1', 'efficientvit_l2', 'efficientvit_m0'\n    pretrained: bool = True\n    in_channels: int = 1\n    input_directory: str = '/kaggle/input'\n    input_model_filename: str = field(init=False)\n    output_model_filename: str = field(init=False)\n    pretrained_model_weights: str = field(init=False)\n    sed_model_weights_path: str = field(init=False)\n    num_classes: str = field(init=False)\n\n    # Focal Loss parameters\n    alpha: float = 0.25 \n    gamma: float = 2.0 \n    reduction: str = \"mean\"\n    bce_weight: float = 1.0\n    focal_weight: float = 1.0\n    secondary_weight: float = .5\n\n    def __post_init__(self):\n        self.metadata_path = os.path.join(self.data_path, 'train.csv')\n        self.taxonomy_path = os.path.join(self.data_path, 'taxonomy.csv')\n        self.sample_submission_path = os.path.join(self.data_path, 'sample_submission.csv')\n        self.location_path = os.path.join(self.data_path, 'recording_location.txt')\n        self.train_data_path = os.path.join(self.data_path, 'train_audio')\n        self.test_soundscapes_path = os.path.join(self.data_path, 'test_soundscapes')\n        self.unlabeled_soundscapes_path = os.path.join(self.data_path, 'train_soundscapes')\n        self.real_spectrograms_metadata_path = os.path.join(self.real_spectrogram_dir, self.real_spectrogram_csv_filename)\n        self.pseudo_spectrograms_metadata_path = os.path.join(self.pseudo_spectrograms_dir, self.pseudo_spectrogram_csv_filename)\n        \n        self.input_model_filename = f'{self.model_name}_pretrained.pth'\n        self.output_model_filename = f'{self.model_name}_sed.pth'\n        self.pretrained_model_weights = os.path.join(self.input_directory, \"offline-packages\", self.input_model_filename)\n        self.sed_model_weights_path = os.path.join(self.input_directory, \"effnet14\", self.output_model_filename)\n        self.num_classes = len(pd.read_csv(self.taxonomy_path))\n\n        self.temporary_dir = tempfile.TemporaryDirectory().name\n        self.spectrogram_time_frames = int((self.FS * self.CHUNK_LENGTH) // self.HOP_LENGTH + 1)\n        if self.debug:\n            self.EPOCHS = 2\n            print(\"⚠️ Debug mode is ON. Training only for 2 epochs.\")\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:04:16.172617Z","iopub.execute_input":"2025-05-21T12:04:16.172854Z","iopub.status.idle":"2025-05-21T12:04:16.279040Z","shell.execute_reply.started":"2025-05-21T12:04:16.172830Z","shell.execute_reply":"2025-05-21T12:04:16.278310Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def set_seed(seed=69):\n    \"\"\"\n    Set seed for reproducibility\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nset_seed(cfg.seed)\nprint(f\"Training with device: {cfg.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:04:16.279856Z","iopub.execute_input":"2025-05-21T12:04:16.280157Z","iopub.status.idle":"2025-05-21T12:04:16.293850Z","shell.execute_reply.started":"2025-05-21T12:04:16.280137Z","shell.execute_reply":"2025-05-21T12:04:16.293161Z"}},"outputs":[{"name":"stdout","text":"Training with device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class EfficientNetSED(nn.Module):\n    \"\"\"\n    EfficientNet with a custom SED head for frequency-wise attention.\n    \n    This model:\n    - Uses a pretrained EfficientNet backbone\n    - Applies a frequency-wise attention mechanism\n    - Outputs class probabilities for multi-class classification\n    \n    Arguments:\n    ----------\n    cfg : object\n        Configuration object (assumes it's an instance of CFG)\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        \n        # Store config and device\n        self.cfg = cfg\n        self.device = torch.device(cfg.device)\n\n        # Create model with the correct architecture\n        self.backbone = timm.create_model(cfg.model_name, pretrained=cfg.pretrained)\n\n        # Load weights manually\n        checkpoint_path = cfg.pretrained_model_weights\n        if checkpoint_path:\n            print(f\"[INFO] Loading weights from {checkpoint_path}\")\n            state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)\n            if \"model\" in state_dict:\n                state_dict = state_dict[\"model\"]  # In case it's wrapped in 'model' key\n            self.backbone.load_state_dict(state_dict)\n\n        # Remove classifier head, we will add our own\n        self.feature_dim = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()  # Remove classifier\n\n        # Frequency-wise attention block -> attention mechanism to emphasize important frequency regions.\n        self.att_block = nn.Sequential(\n            nn.AdaptiveAvgPool2d((None, 1)),          # Mean over frequency bands\n            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n        # Custom classifier head\n        self.classifier = nn.Sequential(\n            nn.Conv2d(self.feature_dim, cfg.num_classes, kernel_size=1),\n            nn.AdaptiveMaxPool2d((1, 1)),\n            nn.Flatten()\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input tensor of shape [B, 3, M, T], where:\n            - B = Batch size\n            - M = Mel bands (frequency bins)\n            - T = Time frames\n\n        Returns:\n        --------\n        torch.Tensor:\n            Output tensor of shape [B, num_classes]\n        \"\"\"\n        x = x.to(self.device)\n        features = self.backbone.forward_features(x)  # EfficientNet backbone [B, C, M', T']\n        attn = self.att_block(features)  # Attention on frequency bands [B, C, T', 1]\n        features = features * attn       # Apply attention\n        \n        out = self.classifier(features)  # Classify [B, num_classes]\n        return out\n\n# Usage Example:\nmodel = EfficientNetSED(cfg)\nmodel = model.to(cfg.device)\n\n# Show model summary\nsample_input = torch.randn(10, 3, cfg.N_MELS, cfg.spectrogram_time_frames).to(cfg.device)  # [Batch, Channels, Mel Bands, Time Frames]\noutput = model(sample_input)\nprint(f\"Model Output Shape: {output.shape}\")  # Should be [10(B), num_classes]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:28:09.560659Z","iopub.execute_input":"2025-05-21T12:28:09.561189Z","iopub.status.idle":"2025-05-21T12:28:11.349122Z","shell.execute_reply.started":"2025-05-21T12:28:09.561160Z","shell.execute_reply":"2025-05-21T12:28:11.348167Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c699784fb84bae8b1482cf8824d00a"}},"metadata":{}},{"name":"stdout","text":"[INFO] Loading weights from /kaggle/input/offline-packages/efficientnet_b0_pretrained.pth\nModel Output Shape: torch.Size([10, 206])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class PrecomputedSpectrogramDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading precomputed log-mel spectrograms.\n\n    Arguments:\n    ----------\n    metadata : pd.DataFrame\n        DataFrame containing filenames, primary labels, and number of frames.\n    spec_dir : str\n        Directory where the spectrogram .npy files are saved.\n    label_to_index : dict\n        Mapping from class name to label index.\n    augment : bool\n        Whether to apply augmentation (placeholder for now).\n    \"\"\"\n    def __init__(self, metadata, spec_dir, augment=False):\n        self.metadata = metadata\n        self.real_spectrogram_dir = os.path.join(spec_dir, \"precomputed_spectograms\")\n        self.label_to_class, self.label_to_index, _, self.filename_to_secondary_label = get_mappings()\n        self.augment = augment\n        if self.augment:\n            self.augmentor = SpectrogramAugmentor()\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        path = os.path.join(self.real_spectrogram_dir, row[\"filename\"])\n        \n        # Load spectrogram\n        spec = np.load(path, mmap_mode=\"r\")\n        spec = torch.tensor(spec, dtype=torch.float16).unsqueeze(0)  # [1, M, T]\n        \n        # Get the class name\n        primary_label = row[\"primary_label\"]\n        class_name = self.label_to_class.get(primary_label, \"Unknown\")\n        \n        # Apply augmentations if enabled\n        if self.augment and class_name in {\"Aves\", \"Insecta\", \"Amphibia\", \"Mammalia\"}:\n            spec = self.augmentor.apply_augmentations(spec, class_name)\n        \n        # Convert to 3-channel image-like format\n        spec = spec.repeat(3, 1, 1)\n        \n        # Multi-hot vector for primary labels\n        primary_label_tensor = torch.zeros(len(self.label_to_index), dtype=torch.float16)\n        if primary_label in self.label_to_index:\n            primary_label_tensor[self.label_to_index[primary_label]] = 1.0\n        \n        # Multi-hot vector for secondary labels\n        secondary_labels = self.filename_to_secondary_label.get(f\"{row['filename'].split('_')[0]}.ogg\", [])\n        secondary_label_tensor = torch.zeros(len(self.label_to_index), dtype=torch.float16)\n        for sec_label in secondary_labels:\n            if sec_label in self.label_to_index:\n                secondary_label_tensor[self.label_to_index[sec_label]] = 0.5\n    \n        # Combine them (primary gets 1.0 weight, secondary gets 0.5 weight)\n        combined_labels = primary_label_tensor + secondary_label_tensor\n        combined_labels = torch.clamp(combined_labels, 0, 1)  # Ensure it's only 0 or 1\n        \n        return {\n            \"spectrogram\": spec, \n            \"labels\": combined_labels, \n            \"filename\": row[\"filename\"], \n            \"class_name\": class_name\n        }\ndef collate_fn(batch, mixup=False, alpha=0.4):\n    \"\"\"\n    Custom collate function to handle varying-size spectrograms and apply Mixup if specified.\n\n    Parameters:\n    -----------\n    batch : list\n        List of samples (dict) with spectrogram, label, and filename.\n    mixup : bool\n        Whether to apply Mixup augmentation to the batch.\n    alpha : float\n        Alpha parameter for the Beta distribution in Mixup.\n\n    Returns:\n    --------\n    dict : \n        Dictionary with stacked tensors and filenames.\n    \"\"\"\n    if len(batch) == 0:\n        return {\"spectrograms\": None, \"labels\": None, \"filenames\": None}\n    \n    # Extract elements\n    specs = [item[\"spectrogram\"] for item in batch]\n    labels = [item[\"labels\"] for item in batch]\n    filenames = [item[\"filename\"] for item in batch]\n\n    # Stack along the batch dimension\n    specs = torch.stack(specs)\n    labels = torch.stack(labels)\n\n    # 🚀 Apply Mixup if specified and more than one element exists in the batch\n    if mixup and len(batch) > 1:\n        indices = torch.randperm(len(batch))\n        mixed_specs = []\n        mixed_labels = []\n\n        for i in range(0, len(batch) - 1, 2):\n            lam = np.random.beta(alpha, alpha)\n            spec1, spec2 = specs[i], specs[indices[i]]\n            label1, label2 = labels[i], labels[indices[i]]\n\n            mixed_spec = lam * spec1 + (1 - lam) * spec2\n            mixed_label = lam * label1 + (1 - lam) * label2\n\n            mixed_specs.append(mixed_spec)\n            mixed_labels.append(mixed_label)\n\n        # If the batch size is odd, we append the last sample as it is\n        if len(batch) % 2 != 0:\n            mixed_specs.append(specs[-1])\n            mixed_labels.append(labels[-1])\n\n        specs = torch.stack(mixed_specs)\n        labels = torch.stack(mixed_labels)\n\n    return {\"spectrograms\": specs, \"labels\": labels, \"filenames\": filenames}\n\ndef get_mappings():\n    \"\"\"\n    Creates label-to-class and label-to-index mappings.\n    Also maps filenames to their associated secondary labels.\n    \n    Returns:\n    --------\n    - label_to_class : dict\n        Maps primary labels to their respective class names.\n    - label_to_index : dict\n        Maps primary labels to unique index values.\n    - filename_to_secondary_label : dict\n        Maps filenames to lists of secondary labels.\n    \"\"\"\n    # Load the datasets\n    taxonomy_df = pd.read_csv(cfg.taxonomy_path)\n    metadata_df = pd.read_csv(cfg.real_spectrograms_metadata_path)\n    train_df = pd.read_csv(cfg.metadata_path)\n    \n    # Filter out unused labels\n    used_labels = set(metadata_df[\"primary_label\"].unique())\n    taxonomy_df = taxonomy_df[taxonomy_df['primary_label'].isin(used_labels)]\n\n    # Label to Class Mapping\n    label_to_class = taxonomy_df.set_index('primary_label')['class_name'].to_dict()\n\n    # Label to Index Mapping → Now guaranteed to match!\n    label_to_index = {label: idx for idx, label in enumerate(sorted(label_to_class.keys()))}\n    index_to_label = {idx: label for idx, label in enumerate(sorted(label_to_class.keys()))}\n\n    # 🌟 New Logic to retrieve filename → secondary_labels\n    filename_to_secondary_label = {}\n\n    # Iterate over the DataFrame\n    for _, row in train_df.iterrows():\n        filename = row[\"filename\"]\n        secondary_labels = eval(row[\"secondary_labels\"]) if isinstance(row[\"secondary_labels\"], str) and row[\"secondary_labels\"] != \"['']\" else []\n\n        # Only add if there are secondary labels\n        if len(secondary_labels) > 0:\n            filename_to_secondary_label[filename] = secondary_labels\n\n    # Display the label maps only once (no need to repeat every call)\n    if not hasattr(get_mappings, \"_displayed\"):\n        tools.display_dataframe_to_user(name=\"Label to Class\", dataframe=pd.DataFrame(label_to_class.items(), columns=[\"Animal Label\", \"Class Name\"]))\n        tools.display_dataframe_to_user(name=\"Label Map\", dataframe=pd.DataFrame(label_to_index.items(), columns=[\"Animal Label\", \"Index\"]))\n        tools.display_dataframe_to_user(name=\"Filename to Secondary Labels\", dataframe=pd.DataFrame(list(filename_to_secondary_label.items()), columns=[\"Filename\", \"Secondary Labels\"]))\n        get_mappings._displayed = True\n    \n    return label_to_class, label_to_index, index_to_label, filename_to_secondary_label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:28:11.350319Z","iopub.execute_input":"2025-05-21T12:28:11.350571Z","iopub.status.idle":"2025-05-21T12:28:11.368794Z","shell.execute_reply.started":"2025-05-21T12:28:11.350554Z","shell.execute_reply":"2025-05-21T12:28:11.368092Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SpectrogramAugmentor:\n    def __init__(self, time_mask_prob=0.3, freq_mask_prob=0.3, mixup_prob=0.3, random_erase_prob=0.5):\n        self.time_mask_prob = time_mask_prob\n        self.freq_mask_prob = freq_mask_prob\n        self.mixup_prob = mixup_prob\n        self.random_erase_prob = random_erase_prob\n\n    def time_mask(self, spec, num_masks=2, max_mask_size=20):\n        \"\"\"Apply Time Masking\"\"\"\n        for _ in range(num_masks):\n            t = random.randint(0, spec.size(2) - 1)\n            mask_size = random.randint(5, max_mask_size)\n            spec[:, :, t:t + mask_size] = 0\n        return spec\n\n    def freq_mask(self, spec, num_masks=2, max_mask_size=20):\n        \"\"\"Apply Frequency Masking\"\"\"\n        for _ in range(num_masks):\n            f = random.randint(0, spec.size(1) - 1)\n            mask_size = random.randint(5, max_mask_size)\n            spec[:, f:f + mask_size, :] = 0\n        return spec\n\n    def random_erasing(self, spec, max_rect=20):\n        \"\"\"Apply Random Erasing\"\"\"\n        if random.random() < self.random_erase_prob:\n            x = random.randint(0, spec.size(2) - max_rect)\n            y = random.randint(0, spec.size(1) - max_rect)\n            w = random.randint(5, max_rect)\n            h = random.randint(5, max_rect)\n            spec[:, y:y+h, x:x+w] = 0\n        return spec\n\n    def mixup(self, spec1, spec2, label1, label2, alpha=0.4):\n        \"\"\"Apply Mixup Augmentation\"\"\"\n        lam = np.random.beta(alpha, alpha)\n        mixed_spec = lam * spec1 + (1 - lam) * spec2\n        mixed_label = lam * label1 + (1 - lam) * label2\n        return mixed_spec, mixed_label\n\n    def apply_augmentations(self, spec, class_name):\n        \"\"\"\n        Conditional augmentations based on class name.\n        \"\"\"\n        if class_name == \"Aves\":  # Birds\n            if random.random() < self.time_mask_prob:\n                spec = self.time_mask(spec)\n            if random.random() < self.freq_mask_prob:\n                spec = self.freq_mask(spec)\n\n        elif class_name == \"Insecta\":  # Insects\n            if random.random() < self.freq_mask_prob:\n                spec = self.freq_mask(spec, max_mask_size=30)\n            if random.random() < self.random_erase_prob:\n                #spec = self.random_erasing(spec, max_rect=30)\n                pass\n\n        elif class_name == \"Amphibia\":  # Amphibians\n            if random.random() < self.time_mask_prob:\n                spec = self.time_mask(spec, max_mask_size=30)\n            if random.random() < self.freq_mask_prob:\n                spec = self.freq_mask(spec)\n\n        elif class_name == \"Mammalia\":  # Mammals\n            if random.random() < self.time_mask_prob:\n                spec = self.time_mask(spec)\n            if random.random() < self.freq_mask_prob:\n                spec = self.freq_mask(spec)\n\n        return spec\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:28:16.049350Z","iopub.execute_input":"2025-05-21T12:28:16.049968Z","iopub.status.idle":"2025-05-21T12:28:16.060586Z","shell.execute_reply.started":"2025-05-21T12:28:16.049943Z","shell.execute_reply":"2025-05-21T12:28:16.059758Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class PseudoLabeledDataset(Dataset):\n    \"\"\"\n    Dataset for pseudo-labeled spectrograms with flexible filtering strategies.\n\n    Supports:\n    - \"none\": raw soft labels\n    - \"hard\": hard thresholding\n    - \"topk\": top-k masking\n    - \"hybrid\": combine hard and soft\n    - callable: pass a function(labels: np.ndarray) -> np.ndarray\n    \"\"\"\n    def __init__(self, metadata_csv, spec_dir, strategy=\"none\", soft_threshold=0.5, hard_threshold=0.8, top_k=1, index_to_label=None):\n        self.df = pd.read_csv(metadata_csv)\n        self.spec_dir = os.path.join(spec_dir, \"pseudo_spectrograms\")\n        self.strategy = strategy\n        self.soft_threshold = soft_threshold\n        self.hard_threshold = hard_threshold\n        self.top_k = top_k\n        self.label_cols = [c for c in self.df.columns if c.startswith(\"class_\")]\n        self.num_classes = len(self.label_cols)\n        self.index_to_label = index_to_label or {i: f\"class_{i}\" for i in range(self.num_classes)}\n        self.filter_fn = strategy if callable(strategy) else None\n        \n        # Precompute label stats on filtered labels\n        self._label_summary = self._compute_label_summary()\n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        spec_path = os.path.join(self.spec_dir, row[\"filename\"])\n        spec = np.load(spec_path).astype(np.float32)\n        spec = torch.tensor(spec).unsqueeze(0).repeat(3, 1, 1)\n\n        labels = row[self.label_cols].values.astype(np.float32)\n\n        # Handle strategy\n        if self.filter_fn:\n            labels = self.filter_fn(labels)\n\n        elif self.strategy == \"hard\":\n            labels = (labels >= self.hard_threshold).astype(np.float32)\n\n        elif self.strategy == \"topk\":\n            topk_idx = labels.argsort()[-self.top_k:]\n            mask = np.zeros_like(labels)\n            mask[topk_idx] = 1.0\n            labels = mask.astype(np.float32)\n\n        elif self.strategy == \"hybrid\":\n            soft_mask = ((labels > self.soft_threshold) & (labels < self.hard_threshold)).astype(np.float32)\n            hard_mask = (labels >= self.hard_threshold).astype(np.float32)\n            labels = labels * soft_mask + 1.0 * hard_mask\n\n        # else: strategy == \"none\", keep raw labels\n\n        return {\n            \"spectrogram\": spec,\n            \"labels\": torch.tensor(labels, dtype=torch.float32),\n            \"filename\": row[\"filename\"]\n        }\n\n    def _compute_label_summary(self, sample_size=500):\n        \"\"\"\n        Samples and summarizes the distribution of label values after filtering.\n        \"\"\"\n        summary = {\n            \"total_samples\": len(self.df),\n            \"avg_labels_per_sample\": 0,\n            \"hard_ones\": 0,\n            \"soft_values\": 0,\n            \"zeros\": 0\n        }\n\n        sample_indices = np.random.choice(len(self.df), size=min(sample_size, len(self.df)), replace=False)\n        label_counts = []\n\n        for idx in sample_indices:\n            row = self.df.iloc[idx]\n            labels = row[self.label_cols].values.astype(np.float32)\n\n            if self.filter_fn:\n                labels = self.filter_fn(labels)\n            elif self.strategy == \"hard\":\n                labels = (labels >= self.hard_threshold).astype(np.float32)\n            elif self.strategy == \"topk\":\n                topk_idx = labels.argsort()[-self.top_k:]\n                mask = np.zeros_like(labels)\n                mask[topk_idx] = 1.0\n                labels = mask.astype(np.float32)\n            elif self.strategy == \"hybrid\":\n                soft_mask = ((labels > self.soft_threshold) & (labels < self.hard_threshold)).astype(np.float32)\n                hard_mask = (labels >= self.hard_threshold).astype(np.float32)\n                labels = labels * soft_mask + 1.0 * hard_mask\n\n            hard = np.sum(labels == 1.0)\n            soft = np.sum((labels > 0) & (labels < 1.0))\n            zeros = np.sum(labels == 0.0)\n\n            label_counts.append(hard + soft)\n            summary[\"hard_ones\"] += hard\n            summary[\"soft_values\"] += soft\n            summary[\"zeros\"] += zeros\n\n        summary[\"avg_labels_per_sample\"] = np.mean(label_counts)\n        summary[\"hard_ones\"] = int(summary[\"hard_ones\"])\n        summary[\"soft_values\"] = int(summary[\"soft_values\"])\n        summary[\"zeros\"] = int(summary[\"zeros\"])\n\n        # Class-wise activations\n        class_stats = {f\"class_{i}\": {\"hard\": 0, \"soft\": 0, \"total\": 0} for i in range(self.num_classes)}\n\n        for idx in sample_indices:\n            row = self.df.iloc[idx]\n            labels = row[self.label_cols].values.astype(np.float32)\n\n            if self.filter_fn:\n                labels = self.filter_fn(labels)\n            elif self.strategy == \"hard\":\n                labels = (labels >= self.hard_threshold).astype(np.float32)\n            elif self.strategy == \"topk\":\n                topk_idx = labels.argsort()[-self.top_k:]\n                mask = np.zeros_like(labels)\n                mask[topk_idx] = 1.0\n                labels = mask.astype(np.float32)\n            elif self.strategy == \"hybrid\":\n                soft_mask = ((labels > self.soft_threshold) & (labels < self.hard_threshold)).astype(np.float32)\n                hard_mask = (labels >= self.hard_threshold).astype(np.float32)\n                labels = labels * soft_mask + 1.0 * hard_mask\n\n            for i, val in enumerate(labels):\n                if val >= 1.0:\n                    class_stats[f\"class_{i}\"][\"hard\"] += 1\n                elif val > 0.0:\n                    class_stats[f\"class_{i}\"][\"soft\"] += 1\n                if val > 0.0:\n                    class_stats[f\"class_{i}\"][\"total\"] += 1\n\n        summary[\"class_stats\"] = class_stats\n\n        return summary\n\n    def __repr__(self):\n        s = f\"PseudoLabeledDataset(strategy='{self.strategy}', size={len(self)}, classes={self.num_classes})\"\n        s += \"\\n\\n  Label Summary (approx. over sample):\"\n        for k, v in self._label_summary.items():\n            if k != \"class_stats\":\n                s += f\"\\n    {k}: {v}\"\n\n        p = \"No thresholds\"\n        if self.strategy==\"hybrid\":\n            p = f\"hard {self.hard_threshold} + soft {self.soft_threshold}\"\n        elif self.strategy==\"topk\":\n            p = f\"topk = {self.top_k}\"\n        elif self.strategy==\"hard\":\n            p = f\"hard {self.hard_threshold}\"\n            \n        s += f\"\\n\\n  Per-Class Activations ({p}):\"\n        stats = self._label_summary[\"class_stats\"]\n        # Show top 10 most frequent\n        s += \"\\n    Top 10 most frequent species:\"\n        n=10\n        most_frequent_stats = sorted(stats.items(), key=lambda x: x[1][\"total\"], reverse=True)[:n]\n        for k, v in most_frequent_stats:\n            label_name = self.index_to_label.get(int(k.replace(\"class_\", \"\")), k)\n            s += f\"\\n      {label_name}: total={v['total']}, hard={v['hard']}, soft={v['soft']}\"\n\n        # Show top 10 less frequent\n        s += \"\\n    Top 10 less frequent species:\"\n        n=10\n        less_frequent_stats = sorted(stats.items(), key=lambda x: x[1][\"total\"], reverse=False)[:n]\n        for k, v in less_frequent_stats:\n            label_name = self.index_to_label.get(int(k.replace(\"class_\", \"\")), k)\n            s += f\"\\n      {label_name}: total={v['total']}, hard={v['hard']}, soft={v['soft']}\"\n        return s\n\ndef get_index_to_label(cfg):\n    taxonomy_df = pd.read_csv(cfg.taxonomy_path)\n    label_to_index = {label: idx for idx, label in enumerate(sorted(taxonomy_df['primary_label'].unique()))}\n    index_to_label = {v: k for k, v in label_to_index.items()}\n    return index_to_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:28:16.254317Z","iopub.execute_input":"2025-05-21T12:28:16.254999Z","iopub.status.idle":"2025-05-21T12:28:16.274965Z","shell.execute_reply.started":"2025-05-21T12:28:16.254977Z","shell.execute_reply":"2025-05-21T12:28:16.274094Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Want to zero out weak predictions, keep the rest as soft?\n\n```python\ndef soft_but_sparse(labels):\n    return np.where(labels > 0.2, labels, 0.0)\n```\n\n```python\ndataset = PseudoLabeledDataset(\n    metadata_csv=\"pseudo_metadata.csv\",\n    spec_dir=\"pseudo_spectrograms\",\n    strategy=soft_but_sparse\n)\n```","metadata":{}},{"cell_type":"code","source":"class CurriculumDatasetWrapper:\n    \"\"\"\n    Wraps multiple datasets and switches between them based on epoch thresholds.\n    Usage:\n        wrapper = CurriculumDatasetWrapper({\n            0: dataset_phase1,\n            5: dataset_phase2,\n            10: dataset_phase3\n        })\n\n        for epoch in range(total_epochs):\n            train_loader = wrapper.get_dataloader(epoch)\n    \"\"\"\n    def __init__(self, phase_datasets: dict, cfg, collate_fn=None):\n        \"\"\"\n        phase_datasets: dict {epoch_threshold: dataset}\n        e.g., {0: real_only, 5: real_plus_high_conf, 10: all_data}\n        \"\"\"\n        self.phases = sorted(phase_datasets.items())  # list of (threshold, dataset)\n        self.cfg = cfg\n        self.collate_fn = collate_fn or (lambda x: collate_fn(x))  # pass-through if None\n        self.current_loader = None\n        self.current_phase = None\n\n    def get_dataloader(self, epoch):\n        selected_dataset = self.phases[0][1]\n        for threshold, dataset in self.phases:\n            if epoch == threshold:\n                selected_dataset = dataset\n    \n        if self.current_phase != selected_dataset:\n            print(f\"[INFO] 🔁 Switching to new curriculum phase at epoch {epoch}\")\n            \n            # ⬇️ Count pseudo vs real samples\n            if isinstance(selected_dataset, ConcatDataset):\n                num_parts = len(selected_dataset.datasets)\n                sizes = [len(d) for d in selected_dataset.datasets]\n                print(f\"[INFO]   Dataset sizes → Real: {sizes[0]}, Pseudo: {sum(sizes[1:])}\")\n            else:\n                print(f\"[INFO]   Dataset size: {len(selected_dataset)}\")\n    \n            self.current_loader = DataLoader(\n                selected_dataset,\n                batch_size=self.cfg.BATCH_SIZE,\n                shuffle=True,\n                num_workers=self.cfg.num_workers,\n                pin_memory=True,\n                collate_fn=self.collate_fn\n            )\n            self.current_phase = selected_dataset\n    \n        return self.current_loader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:18:01.112644Z","iopub.execute_input":"2025-05-21T14:18:01.113382Z","iopub.status.idle":"2025-05-21T14:18:01.120511Z","shell.execute_reply.started":"2025-05-21T14:18:01.113341Z","shell.execute_reply":"2025-05-21T14:18:01.119734Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class FocalLossBCE(nn.Module):\n    def __init__(\n            self,\n            alpha: float = 0.25,\n            gamma: float = 2,\n            reduction: str = \"mean\",\n            bce_weight: float = 1.0,\n            focal_weight: float = 1.0,\n            secondary_weight: float = 0.5,\n    ):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.bce = nn.BCEWithLogitsLoss(reduction=reduction)\n        self.bce_weight = bce_weight\n        self.focal_weight = focal_weight\n        self.secondary_weight = secondary_weight\n\n    def forward(self, logits, targets):\n        # Apply scaling based on presence of primary (1.0) and secondary (0.5) labels\n        weight_scale = targets.clone()\n        weight_scale[weight_scale == self.secondary_weight] = self.secondary_weight\n        weight_scale[weight_scale == 1.0] = 1.0\n\n        focal_loss = sigmoid_focal_loss(\n            inputs=logits,\n            targets=targets,\n            alpha=self.alpha,\n            gamma=self.gamma,\n            reduction=self.reduction,\n        )\n        bce_loss = self.bce(logits, targets)\n        return (self.bce_weight * bce_loss * weight_scale).mean() + (self.focal_weight * focal_loss).mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:18:05.151771Z","iopub.execute_input":"2025-05-21T14:18:05.152654Z","iopub.status.idle":"2025-05-21T14:18:05.158862Z","shell.execute_reply.started":"2025-05-21T14:18:05.152626Z","shell.execute_reply":"2025-05-21T14:18:05.158155Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:18:05.492306Z","iopub.execute_input":"2025-05-21T14:18:05.492860Z","iopub.status.idle":"2025-05-21T14:18:05.498700Z","shell.execute_reply.started":"2025-05-21T14:18:05.492836Z","shell.execute_reply":"2025-05-21T14:18:05.498032Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class Trainer:\n    \"\"\"\n    Trainer class to handle model training and evaluation.\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        The model to be trained.\n    cfg : object\n        Configuration object with training parameters.\n    criterion : torch.nn.Module\n        Loss function.\n    optimizer : torch.optim.Optimizer\n        Optimizer for model updates.\n    scheduler : torch.optim.lr_scheduler\n        Learning rate scheduler.\n    \"\"\"\n    def __init__(self, model, cfg, criterion, optimizer, scheduler):\n        self.model = model.to(cfg.device)\n        self.cfg = cfg\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = cfg.device\n        self.history = []\n        self.scaler = GradScaler()  # ⚡️ Mixed Precision Support\n\n    def train_one_epoch(self, loader):\n        \"\"\"\n        Runs one full epoch of training with tqdm progress bar.\n        \"\"\"\n        self.model.train()\n        total_loss = 0.0\n        all_targets = []\n        all_outputs = []\n\n        for batch in tqdm(loader, desc=\"Training Batch\", leave=False):\n            inputs = batch[\"spectrograms\"].to(self.device)  # [16, 3, 128, 313]\n            targets = batch[\"labels\"].to(self.device)  # [16, 206]\n                \n            # Only keep primary labels for AUC calculation\n            primary_only_targets = (targets == 1).int()  # Convert to float for AUC compatibility\n\n            self.optimizer.zero_grad()\n            with autocast(device_type=self.device):\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n\n            # Scale the loss and backpropagate\n            self.scaler.scale(loss).backward()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n\n            total_loss += loss.item() * inputs.size(0)\n\n            # Collect for AUC calculation\n            all_targets.append(primary_only_targets.cpu().numpy())\n            all_outputs.append(torch.sigmoid(outputs).detach().cpu().numpy())\n\n        # AUC Calculation\n        y_true = np.concatenate(all_targets, axis=0) # (40559, 206)\n        y_pred = np.concatenate(all_outputs, axis=0)\n\n        #print(y_true.shape, y_true.dtype, y_pred.dtype)\n        assert y_true.shape == y_pred.shape\n        assert y_true.shape[1] == self.cfg.num_classes\n        #assert y_true.dtype == np.int32\n        #assert y_pred.dtype == np.float16\n        #assert set(np.unique(y_true)).issubset({0, 1})\n        \n        macro_auc, class_wise = self.calculate_auc(y_true, y_pred)\n\n\n        return total_loss / len(loader.dataset), macro_auc\n        \n    def evaluate(self, loader):\n        \"\"\"\n        Evaluates the model on the validation set with tqdm progress bar.\n        Returns:\n            - macro_auc: mean ROC AUC over valid classes\n            - per_class_auc: dict of {class_index: AUC}\n        \"\"\"\n        self.model.eval()\n        all_targets = []\n        all_outputs = []\n    \n        with torch.no_grad():\n            for batch in tqdm(loader, desc=\"Evaluating Batch\", leave=False): # dict_keys(['spectrogram', 'labels', 'filename'])\n                inputs = batch[\"spectrogram\"].to(self.device)\n                targets = batch[\"labels\"].to(self.device).float()\n    \n                primary_only_targets = (targets == 1).int()\n    \n                with autocast(device_type=self.device):\n                    outputs = self.model(inputs)\n                    outputs = torch.sigmoid(outputs).detach().cpu().numpy()\n    \n                all_targets.append(primary_only_targets.cpu().numpy())\n                all_outputs.append(outputs)\n    \n        y_true = np.concatenate(all_targets, axis=0)\n        y_pred = np.concatenate(all_outputs, axis=0)\n    \n        macro_auc, class_wise = self.calculate_auc(y_true, y_pred)\n        return macro_auc, class_wise\n            \n    def calculate_auc(self, y_true, y_pred):\n        \"\"\"\n        Calculates the macro ROC-AUC score.\n        \n        Parameters:\n        -----------\n        y_true : np.ndarray\n            Ground truth binary labels (multi-hot encoded).\n        y_pred : np.ndarray\n            Model predictions (probabilities).\n        \n        Returns:\n        --------\n        float:\n            Macro ROC-AUC score.\n        \"\"\"\n        scores = []\n        class_scores = {}\n        for i in range(y_true.shape[1]):\n            if 0 < np.sum(y_true[:, i]) < len(y_true):\n                try:\n                    auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n                    scores.append(auc)\n                    class_scores[i] = auc\n                except ValueError as e:\n                    print(f\"[WARNING] ValueError during AUC calculation for class index: {i}\")\n                    print(e)\n                    class_scores[i] = None\n            else:\n                class_scores[i] = None\n        return np.mean(scores) if scores else 0.0, class_scores\n\n\n\n    def fit(self, train_loader, val_loader=None, curriculum=None, val_phase_map=None):\n        \"\"\"\n        Runs the full training loop with tqdm progress bar.\n        \"\"\"\n        best_val_auc = 0.0  # for optional checkpointing\n        print(f\"[INFO] Starting training... at {time.strftime('%H:%M:%S')}\")\n        for epoch in tqdm(range(self.cfg.EPOCHS), total=self.cfg.EPOCHS, desc=\"Training Epoch\"):\n            start_time = time.time()\n\n            if train_loader is None:\n                train_loader = curriculum.get_dataloader(epoch)\n                val_loader = val_phase_map.get(epoch, None)\n\n            # Train and Evaluate\n            train_loss, train_auc = self.train_one_epoch(train_loader)\n            \n            if val_loader is not None:\n                val_auc, per_class = self.evaluate(val_loader)\n                # Record history\n                self.history.append((epoch + 1, train_loss, train_auc, val_auc))\n                print(f\"Epoch {epoch + 1}/{self.cfg.EPOCHS} - \"\n                      f\"Loss: {train_loss:.4f} - \"\n                      f\"Train AUC: {train_auc:.4f} - \"\n                      f\"Val AUC: {val_auc:.4f} - \"\n                      f\"Time: {time.time() - start_time:.1f}s\")\n\n                # Print top N underperforming classes (optional)\n                underperformers = sorted([(k, v) for k, v in per_class.items() if v is not None], key=lambda x: x[1])[:5]\n                print(\"[INFO] 🔍 Worst classes by AUC:\")\n                for class_idx, auc in underperformers:\n                    print(f\"    class_{class_idx}: AUC = {auc:.4f}\")\n    \n                # === Optional checkpointing (uncomment to enable) ===\n                if val_auc > best_val_auc:\n                    print(f\"[INFO] ✅ New best AUC: {val_auc:.4f}, saving checkpoint...\")\n                    best_val_auc = val_auc\n                    torch.save(self.model.state_dict(), os.path.join(self.cfg.OUTPUT_DIR, \"best_model.pth\"))\n            \n            else:\n                # Record history\n                self.history.append((epoch + 1, train_loss, train_auc))\n                print(f\"Epoch {epoch + 1}/{self.cfg.EPOCHS} - \"\n                      f\"Loss: {train_loss:.4f} - \"\n                      f\"Train AUC: {train_auc:.4f} - \"\n                      f\"Time: {time.time() - start_time:.1f}s\")\n\n            # Scheduler step\n            self.scheduler.step()\n            train_loader = None\n\n        # Display history as a DataFrame\n        col_names = [\"Epoch\", \"Train Loss\", \"Train ROC-AUC\", \"Val ROC-AUC\"] if val_loader else [\"Epoch\", \"Train Loss\", \"Train ROC-AUC\"]\n        tools.display_dataframe_to_user(\n            name=\"Training History\", \n            dataframe=pd.DataFrame(self.history, columns=col_names)\n        )\n        self.save_model()\n\n    def save_model(self):\n        \"\"\"\n        Saves the model checkpoint.\n        \"\"\"\n        checkpoint_path = os.path.join(self.cfg.OUTPUT_DIR, self.cfg.output_model_filename)\n        torch.save(self.model.state_dict(), checkpoint_path)\n        print(f\"[INFO] Model saved to {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:34:53.653149Z","iopub.execute_input":"2025-05-21T14:34:53.653884Z","iopub.status.idle":"2025-05-21T14:34:53.672706Z","shell.execute_reply.started":"2025-05-21T14:34:53.653856Z","shell.execute_reply":"2025-05-21T14:34:53.672004Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# === Load your trained model ===\nmodel = EfficientNetSED(cfg)\nprint(f\"[INFO] Loading weights from {cfg.sed_model_weights_path}\")\nmodel.load_state_dict(torch.load(cfg.sed_model_weights_path, map_location=cfg.device))\nmodel.to(cfg.device)\n\n# === Define label mappings ===\n_, label_to_index, index_to_label, _ = get_mappings()\n\n# === Define datasets ===\nmetadata = pd.read_csv(cfg.real_spectrograms_metadata_path)\ndataset_real = PrecomputedSpectrogramDataset(metadata, cfg.real_spectrogram_dir, augment=cfg.augment)\n\ndataset_pseudo_80 = PseudoLabeledDataset(\n    metadata_csv=cfg.pseudo_spectrograms_metadata_path,\n    spec_dir=cfg.pseudo_spectrograms_dir,\n    strategy=\"hybrid\",  # or \"hard\"\n    hard_threshold=0.25,\n    soft_threshold=0.10,\n    index_to_label=index_to_label\n)\n\ndataset_pseudo_60 = PseudoLabeledDataset(\n    metadata_csv=cfg.pseudo_spectrograms_metadata_path,\n    spec_dir=cfg.pseudo_spectrograms_dir,\n    strategy=\"hybrid\",  \n    hard_threshold=0.18,\n    soft_threshold=0.05,\n    index_to_label=index_to_label\n)\n\ndataset_pseudo_40 = PseudoLabeledDataset(\n    metadata_csv=cfg.pseudo_spectrograms_metadata_path,\n    spec_dir=cfg.pseudo_spectrograms_dir,\n    strategy=\"hybrid\",  \n    hard_threshold=0.1,\n    soft_threshold=0.01,\n    index_to_label=index_to_label\n)\n# === Combine datasets in curriculum ===\ncurriculum = CurriculumDatasetWrapper({\n    0: dataset_real,\n    2: ConcatDataset([dataset_real, dataset_pseudo_80]),\n    8: ConcatDataset([dataset_real, dataset_pseudo_60]),\n    14: ConcatDataset([dataset_real, dataset_pseudo_40])\n}, cfg, collate_fn=lambda x: collate_fn(x))\n\n# Add these to your pipeline setup\nval_phase_map = {\n    1: DataLoader(dataset_pseudo_80, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=cfg.num_workers),\n    7: DataLoader(dataset_pseudo_60, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=cfg.num_workers),\n    6: DataLoader(dataset_pseudo_60, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=cfg.num_workers),\n    12: DataLoader(dataset_pseudo_40, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=cfg.num_workers),\n    13: DataLoader(dataset_pseudo_40, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=cfg.num_workers),\n}\n# Loss, Optimizer, and Scheduler\ncriterion = FocalLossBCE(alpha=cfg.alpha, gamma=cfg.gamma, reduction=cfg.reduction, bce_weight=cfg.bce_weight, focal_weight=cfg.focal_weight)\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE)\n# Define the base scheduler (Cosine Annealing)\ncosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.EPOCHS, eta_min=cfg.min_lr)\n\n# Define the warmup scheduler with a 5-epoch warmup\nscheduler = GradualWarmupSchedulerV2(\n    optimizer,\n    multiplier=1,\n    total_epoch=5,    # Number of epochs to warm up\n    after_scheduler=cosine_scheduler\n)\n# Initialize trainer\ntrainer = Trainer(model, cfg, criterion, optimizer, scheduler)\ntrainer.fit(train_loader=None, curriculum=curriculum, val_phase_map=val_phase_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T14:34:54.001672Z","iopub.execute_input":"2025-05-21T14:34:54.001915Z","execution_failed":"2025-05-21T19:04:55.140Z"}},"outputs":[{"name":"stdout","text":"[INFO] Loading weights from /kaggle/input/offline-packages/efficientnet_b0_pretrained.pth\n[INFO] Loading weights from /kaggle/input/effnet14/efficientnet_b0_sed.pth\n[INFO] Starting training... at 14:35:32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53ffe2ee9c954b71a2658c972ad80cbf"}},"metadata":{}},{"name":"stdout","text":"[INFO] 🔁 Switching to new curriculum phase at epoch 0\n[INFO]   Dataset size: 81117\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Batch:   0%|          | 0/2535 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/20 - Loss: 0.0008 - Train AUC: 0.9929 - Time: 263.8s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Batch:   0%|          | 0/2535 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating Batch:   0%|          | 0/6991 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a4f242bed1e47c598f2634c66dfddbb"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"class HardNegativeCollector:\n    def __init__(self, model, dataset, cfg, threshold=0.5, max_per_class=100, output_csv=\"hard_negatives.csv\"):\n        self.model = model.eval()\n        self.dataset = dataset\n        self.cfg = cfg\n        self.threshold = threshold\n        self.max_per_class = max_per_class\n        self.output_csv = os.path.join(cfg.OUTPUT_DIR, output_csv)\n\n    def collect(self):\n        device = self.cfg.device\n        loader = DataLoader(self.dataset, batch_size=self.cfg.BATCH_SIZE, shuffle=False,\n                            num_workers=self.cfg.num_workers, pin_memory=True)\n\n        hard_examples = []\n\n        with torch.no_grad():\n            for batch in tqdm(loader, desc=\"⛏ Collecting Hard Negatives\"):\n                inputs = batch[\"spectrograms\"].to(device)\n                filenames = batch[\"filename\"]\n                true = batch[\"labels\"].cpu().numpy()\n                with autocast(device_type=device):\n                    pred = self.model(inputs)\n                    pred = torch.sigmoid(pred).detach().cpu().numpy()\n\n                for i in range(len(filenames)):\n                    filename = filenames[i]\n                    y_true = true[i]\n                    y_pred = pred[i]\n\n                    # Identify FPs and FNs\n                    for class_idx in range(len(y_true)):\n                        t, p = y_true[class_idx], y_pred[class_idx]\n\n                        if t == 1 and p < self.threshold:\n                            hard_examples.append({\n                                \"filename\": filename,\n                                \"class_index\": class_idx,\n                                \"error_type\": \"false_negative\",\n                                \"confidence\": p\n                            })\n\n                        elif t == 0 and p >= self.threshold:\n                            hard_examples.append({\n                                \"filename\": filename,\n                                \"class_index\": class_idx,\n                                \"error_type\": \"false_positive\",\n                                \"confidence\": p\n                            })\n\n        # Build DataFrame\n        df = pd.DataFrame(hard_examples)\n\n        # Optionally limit per class\n        df_limited = df.groupby([\"class_index\", \"error_type\"]).head(self.max_per_class).reset_index(drop=True)\n\n        df_limited.to_csv(self.output_csv, index=False)\n        print(f\"[INFO] 🔍 Saved {len(df_limited)} hard negatives to {self.output_csv}\")\n        return df_limited\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-21T19:04:55.141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metadata = pd.read_csv(cfg.real_spectrograms_metadata_path)\ndataset_real = PrecomputedSpectrogramDataset(metadata, cfg.real_spectrogram_dir, augment=False)\n\nhnm_collector = HardNegativeCollector(\n    model=model,\n    dataset=dataset_real,\n    cfg=cfg,\n    threshold=0.5,\n    max_per_class=100,\n    output_csv=\"hard_negatives.csv\"\n)\n\nhard_df = hnm_collector.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HardNegativeDataset(Dataset):\n    \"\"\"\n    Dataset for training on hard negatives (false positives / false negatives).\n    \n    Arguments:\n    ----------\n    hard_csv : str\n        Path to the CSV file with hard mistakes (from HardNegativeCollector).\n    spec_dir : str\n        Directory where the precomputed spectrogram .npy files are stored.\n    num_classes : int\n        Total number of output classes (206 for this task).\n    filter_error_type : str or None\n        If specified, filter to only \"false_negative\" or \"false_positive\".\n    label_weight : float\n        Value to assign to the hard mistake class (e.g., 1.0).\n    \"\"\"\n    def __init__(self, hard_csv, spec_dir, num_classes, filter_error_type=None, label_weight=1.0):\n        self.df = pd.read_csv(hard_csv)\n        self.spec_dir = spec_dir\n        self.num_classes = num_classes\n        self.filter_error_type = filter_error_type\n        self.label_weight = label_weight\n\n        if self.filter_error_type:\n            self.df = self.df[self.df[\"error_type\"] == self.filter_error_type].reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        spec_path = os.path.join(self.spec_dir, row[\"filename\"])\n        spec = np.load(spec_path).astype(np.float32)\n        spec = torch.tensor(spec).unsqueeze(0).repeat(3, 1, 1)\n\n        label = torch.zeros(self.num_classes, dtype=torch.float32)\n        label[int(row[\"class_index\"])] = self.label_weight\n\n        return {\n            \"spectrogram\": spec,\n            \"labels\": label,\n            \"filename\": row[\"filename\"],\n            \"class_index\": int(row[\"class_index\"]),\n            \"error_type\": row[\"error_type\"]\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hard_dataset = HardNegativeDataset(\n    hard_csv=os.path.join(cfg.OUTPUT_DIR, \"hard_negatives.csv\"),\n    spec_dir=cfg.real_spectrogram_dir,\n    num_classes=cfg.num_classes,\n    filter_error_type=None,  # or \"false_negative\", or \"false_positive\"\n    label_weight=1.0\n)\n\nhard_loader = create_dataloader(hard_dataset, cfg, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}